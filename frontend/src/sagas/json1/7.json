{"topic": "<ul class=\"tags\">\n<a class=\"tag\" href=\"/tags#GD\">GD</a>\n<a class=\"tag\" href=\"/tags#Optimization\">Optimization</a>\n</ul>", "title": "<h1 class=\"post-title\" itemprop=\"name\">Bài 7: Gradient Descent (phần 1/2)</h1>", "introduction": {"title": "<h2 id=\"1-giới-thiệu\">1. Giới thiệu</h2>", "content": "<p>Các bạn hẳn thấy hình vẽ dưới đây quen thuộc:</p><div class=\"imgcap\">\n<img align=\"center\" src=\"https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/GD/gradient_descent.png?raw=true\" width=\"600\"/>\n</div><p>Điểm màu xanh lục là điểm local minimum (cực tiểu), và cũng là điểm làm cho hàm\nsố đạt giá trị nhỏ nhất. Từ đây trở đi, tôi sẽ dùng <em>local minimum</em> để thay cho\n<em>điểm cực tiểu</em>, <em>global minimum</em> để thay cho <em>điểm mà tại đó hàm số đạt giá trị\nnhỏ nhất</em>. Global minimum là một trường hợp đặc biệt của local minimum.</p><p>Giả sử chúng ta đang quan tâm đến một hàm số một biến có đạo hàm mọi nơi. Xin\ncho tôi được nhắc lại vài điều đã quá quen thuộc:</p><ol>\n<li>\n<p>Điểm local minimum \\(x^*\\) của hàm số là điểm có đạo hàm \\(f’(x^*)\\)\nbằng 0. Hơn thế nữa, trong lân cận của nó, đạo hàm của các điểm phía bên trái\n\\(x^*\\) là không dương, đạo hàm của các điểm phía bên phải \\(x^*\\) là\nkhông âm.</p>\n</li>\n<li>\n<p>Đường tiếp tuyến với đồ thị hàm số đó tại 1 điểm bất kỳ có hệ số góc chính\nbằng đạo hàm của hàm số tại điểm đó.</p>\n</li>\n</ol><p>Trong hình phía trên, các điểm bên trái của điểm local minimum màu xanh lục có\nđạo hàm âm, các điểm bên phải có đạo hàm dương. Và đối với hàm số này, càng xa\nvề phía trái của điểm local minimum thì đạo hàm càng âm, càng xa về phía phải\nthì đạo hàm càng dương.</p><p><a name=\"gradient-descent\"></a></p><h3 id=\"gradient-descent\">Gradient Descent</h3><p>Trong Machine Learning nói riêng và Toán Tối Ưu nói chung, chúng ta thường xuyên\nphải tìm giá trị nhỏ nhất (hoặc đôi khi là lớn nhất) của một hàm số nào đó. Ví\ndụ như các hàm mất mát trong hai bài <a href=\"/2016/12/28/linearregression/\">Linear Regression</a> \nvà <a href=\"/2017/01/01/kmeans/\">K-means Clustering</a>. Nhìn chung, việc tìm global\nminimum của các hàm mất mát trong Machine Learning là rất phức tạp, thậm chí là\nbất khả thi. Thay vào đó, người ta thường cố gắng tìm các điểm local minimum, và\nở một mức độ nào đó, coi đó là nghiệm cần tìm của bài toán.</p><p>Các điểm local minimum là nghiệm của phương trình đạo hàm bằng 0. Nếu bằng một\ncách nào đó có thể tìm được toàn bộ (hữu hạn) các điểm cực tiểu, ta chỉ cần thay\ntừng điểm local minimum đó vào hàm số rồi tìm điểm làm cho hàm có giá trị nhỏ\nnhất (<em>đoạn này nghe rất quen thuộc, đúng không?</em>). Tuy nhiên, trong hầu hết các\ntrường hợp, việc giải phương trình đạo hàm bằng 0 là bất khả thi. Nguyên nhân có\nthể đến từ sự phức tạp của dạng của đạo hàm, từ việc các điểm dữ liệu có số\nchiều lớn, hoặc từ việc có quá nhiều điểm dữ liệu.</p><p>Hướng tiếp cận phổ biến nhất là xuất phát từ một điểm mà chúng ta coi là <em>gần</em>\nvới nghiệm của bài toán, sau đó dùng một phép toán lặp để <em>tiến dần</em> đến điểm\ncần tìm, tức đến khi đạo hàm gần với 0. Gradient Descent (viết gọn là GD) và các\nbiến thể của nó là một trong những phương pháp được dùng nhiều nhất.</p><p><a name=\"large-scale\"></a></p><p>Vì kiến thức về GD khá rộng nên tôi xin phép được chia thành hai phần. Phần 1\nnày giới thiệu ý tưởng phía sau thuật toán GD và một vài ví dụ đơn giản giúp các\nbạn làm quen với thuật toán này và vài khái niệm mới. Phần 2 sẽ nói về các\nphương pháp cải tiến GD và các biến thể của GD trong các bài toán mà số chiều và\nsố điểm dữ liệu lớn. Những bài toán như vậy được gọi là <em>large-scale</em>.</p><p><a name=\"-gradient-descent-cho-ham--bien\"></a></p>"}, "formulas": {"title": "<h2 id=\"2-gradient-descent-cho-hàm-1-biến\">2. Gradient Descent cho hàm 1 biến</h2>", "content": "<p>Quay trở lại hình vẽ ban đầu và một vài quan sát tôi đã nêu. Giả sử\n\\(x_{t}\\) là điểm ta tìm được sau vòng lặp thứ \\(t\\). Ta cần tìm một thuật\ntoán để đưa \\(x_{t}\\) về càng gần \\(x^*\\) càng tốt.</p><p>Trong hình đầu tiên, chúng ta lại có thêm hai quan sát nữa:</p><ol>\n<li>\n<p>Nếu đạo hàm của hàm số tại \\(x_{t}\\): \\(f’(x_{t}) &gt; 0\\) thì\n\\(x_{t}\\) nằm về bên phải so với \\(x^*\\) (và ngược lại). Để điểm tiếp\ntheo \\(x_{t+1}\\) gần với \\(x^*\\) hơn, chúng ta cần di chuyển\n\\(x_{t}\\) về phía bên trái, tức về phía <em>âm</em>. Nói các khác, <strong>chúng ta cần\ndi chuyển ngược dấu với đạo hàm</strong>:\n\\[\nx_{t+1} = x_{t} + \\Delta\n\\]\nTrong đó \\(\\Delta\\) là một đại lượng ngược dấu với đạo hàm \\(f’(x_{t})\\).</p>\n</li>\n<li>\n<p>\\(x_{t}\\) càng xa \\(x^*\\) về phía bên phải thì \\(f’(x_{t})\\) càng lớn\nhơn 0 (và ngược lại). Vậy, lượng di chuyển \\(\\Delta\\), một cách trực quan\nnhất, là tỉ lệ thuận với \\(-f’(x_{t})\\).</p>\n</li>\n</ol><p>Hai nhận xét phía trên cho chúng ta một cách cập nhật đơn giản là:\n\\[\nx_{t+1} = x_{t} - \\eta f’(x_{t})\n\\]</p><p>Trong đó \\(\\eta\\) (đọc là <em>eta</em>) là một số dương được gọi là <em>learning rate</em>\n(tốc độ học). Dấu trừ thể hiện việc chúng ta phải <em>đi ngược</em> với đạo hàm (Đây\ncũng chính là lý do phương pháp này được gọi là Gradient Descent - <em>descent</em>\nnghĩa là <em>đi ngược</em>). Các quan sát đơn giản phía trên, mặc dù không phải đúng\ncho tất cả các bài toán, là nền tảng cho rất nhiều phương pháp tối ưu nói chung\nvà thuật toán Machine Learning nói riêng.</p><p><a name=\"vi-du-don-gian-voi-python\"></a></p><h3 id=\"ví-dụ-đơn-giản-với-python\">Ví dụ đơn giản với Python</h3><p>Xét hàm số \\(f(x) = x^2 + 5\\sin(x)\\) với đạo hàm \\(f’(x) = 2x + 5\\cos(x)\\)\n(một lý do tôi chọn hàm này vì nó không dễ tìm nghiệm của đạo hàm bằng 0 như hàm\nphía trên). Giả sử bắt đầu từ một điểm \\(x_{0}\\) nào đó, tại vòng lặp thứ\n\\(t\\), chúng ta sẽ cập nhật như sau:\n\\[\nx_{t+1} = x_{t} - \\eta(2x_{t} + 5\\cos(x_{t}))\n\\]</p><p>Như thường lệ, tôi khai báo vài thư viện quen thuộc</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># To support both python 2 and python 3\n</span><span class=\"kn\">from</span> <span class=\"nn\">__future__</span> <span class=\"kn\">import</span> <span class=\"n\">division</span><span class=\"p\">,</span> <span class=\"n\">print_function</span><span class=\"p\">,</span> <span class=\"n\">unicode_literals</span>\n<span class=\"kn\">import</span> <span class=\"nn\">math</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span> \n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n</code></pre></div></div><p>Tiếp theo, tôi viết các hàm số :</p><ol>\n<li><code class=\"language-plaintext highlighter-rouge\">grad</code> để tính đạo hàm</li>\n<li><code class=\"language-plaintext highlighter-rouge\">cost</code> để tính giá trị của hàm số. Hàm này không sử dụng trong thuật toán\nnhưng thường được dùng để kiểm tra việc tính đạo hàm của đúng không hoặc để\nxem giá trị của hàm số có giảm theo mỗi vòng lặp hay không.</li>\n<li><code class=\"language-plaintext highlighter-rouge\">myGD1</code> là phần chính thực hiện thuật toán Gradient Desent nêu phía trên. Đầu\nvào của hàm số này là learning rate và điểm bắt đầu. Thuật toán dừng lại khi\nđạo hàm có độ lớn đủ nhỏ.</li>\n</ol><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">grad</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"o\">+</span> <span class=\"mi\">5</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">cos</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">cost</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">x</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"mi\">5</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">myGD1</span><span class=\"p\">(</span><span class=\"n\">eta</span><span class=\"p\">,</span> <span class=\"n\">x0</span><span class=\"p\">):</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">x0</span><span class=\"p\">]</span>\n    <span class=\"k\">for</span> <span class=\"n\">it</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n        <span class=\"n\">x_new</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">eta</span><span class=\"o\">*</span><span class=\"n\">grad</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n        <span class=\"k\">if</span> <span class=\"nb\">abs</span><span class=\"p\">(</span><span class=\"n\">grad</span><span class=\"p\">(</span><span class=\"n\">x_new</span><span class=\"p\">))</span> <span class=\"o\">&lt;</span> <span class=\"mf\">1e-3</span><span class=\"p\">:</span>\n            <span class=\"k\">break</span>\n        <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">x_new</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">it</span><span class=\"p\">)</span>\n</code></pre></div></div><p><a name=\"diem-khoi-tao-khac-nhau\"></a></p><h4 id=\"điểm-khởi-tạo-khác-nhau\">Điểm khởi tạo khác nhau</h4><p>Sau khi có các hàm cần thiết, tôi thử tìm nghiệm với các điểm khởi tạo khác nhau\nlà \\(x_{0} = -5\\) và \\(x_{0} = 5\\).</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">(</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">it1</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">myGD1</span><span class=\"p\">(.</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"p\">(</span><span class=\"n\">x2</span><span class=\"p\">,</span> <span class=\"n\">it2</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">myGD1</span><span class=\"p\">(.</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Solution x1 = %f, cost = %f, obtained after %d iterations'</span><span class=\"o\">%</span><span class=\"p\">(</span><span class=\"n\">x1</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">cost</span><span class=\"p\">(</span><span class=\"n\">x1</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">it1</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Solution x2 = %f, cost = %f, obtained after %d iterations'</span><span class=\"o\">%</span><span class=\"p\">(</span><span class=\"n\">x2</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">cost</span><span class=\"p\">(</span><span class=\"n\">x2</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">it2</span><span class=\"p\">))</span>\n</code></pre></div></div><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Solution x1 = -1.110667, cost = -3.246394, obtained after 11 iterations\nSolution x2 = -1.110341, cost = -3.246394, obtained after 29 iterations\n</code></pre></div></div><p>Vậy là với các điểm ban đầu khác nhau, thuật toán của chúng ta tìm được nghiệm\ngần giống nhau, mặc dù với tốc độ hội tụ khác nhau. Dưới đây là hình ảnh minh\nhọa thuật toán GD cho bài toán này (<em>xem tốt trên Desktop ở chế độ full màn\nhình</em>).</p><table style=\"border: 0px solid white\" width=\"100%\">\n<tr>\n<td style=\"border: 0px solid white\" width=\"50%\">\n<img src=\"/assets/GD/1dimg_5_0.1_-5.gif\" style=\"display:block;\" width=\"100%\"/>\n</td>\n<td style=\"border: 0px solid white\" width=\"50%\">\n<img src=\"/assets/GD/1dimg_5_0.1_5.gif\" style=\"display:block;\" width=\"100%\"/>\n</td>\n</tr>\n</table><p>Từ hình minh họa trên ta thấy rằng ở hình bên trái, tương ứng với \\(x_{0} = -5\\), nghiệm hội tụ nhanh hơn, vì điểm ban đầu \\(x_0\\) gần với nghiệm \\( x^* \\approx -1\\)  hơn. Hơn nữa, với \\(x_{0} = 5 \\) ở hình bên phải, <em>đường đi</em> của nghiệm có chứa một khu vực có đạo hàm khá nhỏ gần điểm có hoành độ bằng 2. Điều này khiến cho thuật toán <em>la cà</em> ở đây khá lâu. Khi vượt qua được điểm này thì mọi việc diễn ra rất tốt đẹp.</p><p><a name=\"learning-rate-khac-nhau\"></a></p><h4 id=\"learning-rate-khác-nhau\">Learning rate khác nhau</h4><p>Tốc độ hội tụ của GD không những phụ thuộc vào điểm khởi tạo ban đầu mà còn phụ thuộc vào <em>learning rate</em>. Dưới đây là một ví dụ với cùng điểm khởi tạo \\(x_{0} = -5\\) nhưng learning rate khác nhau:</p><table style=\"border: 0px solid white\" width=\"100%\">\n<tr>\n<td style=\"border: 0px solid white\" width=\"50%\">\n<img src=\"/assets/GD/1dimg_5_0.01_-5.gif\" style=\"display:block;\" width=\"100%\"/>\n</td>\n<td style=\"border: 0px solid white\" width=\"50%\">\n<img src=\"/assets/GD/1dimg_5_0.5_-5.gif\" style=\"display:block;\" width=\"100%\"/>\n</td>\n<!-- <td width=\"30%\" style = \"border: 0px solid white\">\n            <img src = \"/assets/GD/1dimg_5_0.5_5.gif\">\n        </td> -->\n</tr>\n</table><p>Ta quan sát thấy hai điều:</p><ol>\n<li>Với <em>learning rate</em> nhỏ \\(\\eta = 0.01\\), tốc độ hội tụ rất chậm. Trong ví\ndụ này tôi chọn tối đa 100 vòng lặp nên thuật toán dừng lại trước khi tới\n<em>đích</em>, mặc dù đã rất gần. Trong thực tế, khi việc tính toán trở nên phức\ntạp, <em>learning rate</em> quá thấp sẽ ảnh hưởng tới tốc độ của thuật toán rất\nnhiều, thậm chí không bao giờ tới được đích.</li>\n<li>Với <em>learning rate</em> lớn \\(\\eta = 0.5\\), thuật toán tiến rất nhanh tới <em>gần\nđích</em> sau vài vòng lặp. Tuy nhiên, thuật toán không hội tụ được vì <em>bước\nnhảy</em> quá lớn, khiến nó cứ <em>quẩn quanh</em> ở đích.</li>\n</ol><p>Việc lựa chọn <em>learning rate</em> rất quan trọng trong các bài toán thực tế. Việc\nlựa chọn giá trị này phụ thuộc nhiều vào từng bài toán và phải làm một vài thí\nnghiệm để chọn ra giá trị tốt nhất. Ngoài ra, tùy vào một số bài toán, GD có thể\nlàm việc hiệu quả hơn bằng cách chọn ra <em>learning rate</em> phù hợp hoặc chọn\n<em>learning rate</em> khác nhau ở mỗi vòng lặp. Tôi sẽ quay lại vấn đề này ở phần 2.</p><p><a name=\"-gradient-descent-cho-ham-nhieu-bien\"></a></p>"}, "examples": {"title": "<h2 id=\"3-gradient-descent-cho-hàm-nhiều-biến\">3. Gradient Descent cho hàm nhiều biến</h2>", "content": "<p>Giả sử ta cần tìm global minimum cho hàm \\(f(\\mathbf{\\theta})\\) trong đó\n\\(\\mathbf{\\theta}\\) (<em>theta</em>) là một vector, thường được dùng để ký hiệu tập\nhợp các tham số của một mô hình cần tối ưu (trong Linear Regression thì các tham\nsố chính là hệ số \\(\\mathbf{w}\\)). Đạo hàm của hàm số đó tại một điểm\n\\(\\theta\\) bất kỳ được ký hiệu là \\(\\nabla_{\\theta}f(\\theta)\\) (hình tam\ngiác ngược đọc là <em>nabla</em>). Tương tự như hàm 1 biến, thuật toán GD cho hàm nhiều\nbiến cũng bắt đầu bằng một điểm dự đoán \\(\\theta_{0}\\), sau đó, ở vòng lặp\nthứ \\(t\\), quy tắc cập nhật là:</p><p>\\[\n\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_{\\theta} f(\\theta_{t})\n\\]</p><p>Hoặc viết dưới dạng đơn giản hơn: \\(\\theta = \\theta - \\eta \\nabla_{\\theta} f(\\theta)\\).</p><p>Quy tắc cần nhớ: <strong>luôn luôn đi ngược hướng với đạo hàm</strong>.</p><p>Việc tính toán đạo hàm của các hàm nhiều biến là một kỹ năng cần thiết. Một vài đạo hàm đơn giản có thể được <a href=\"/math/#bang-cac-dao-ham-co-ban\">tìm thấy ở đây</a>.\n<a name=\"quay-lai-voi-bai-toan-linear-regression\"></a></p><h3 id=\"quay-lại-với-bài-toán-linear-regression\">Quay lại với bài toán Linear Regression</h3><p>Trong mục này, chúng ta quay lại với bài toán <a href=\"/2016/12/28/linearregression/\">Linear Regression</a> và thử tối ưu hàm mất mát của nó bằng thuật toán GD.</p><p>Hàm mất mát của Linear Regression là: \n\\[\n\\mathcal{L}(\\mathbf{w}) = \\frac{1}{2N}||\\mathbf{y - \\bar{X}w}||_2^2\n\\]</p><p><strong>Chú ý</strong>: hàm này có khác một chút so với hàm tôi nêu trong bài <a href=\"/2016/12/28/linearregression/\">Linear Regression</a>. Mẫu số có thêm \\(N\\) là số lượng dữ liệu trong training set. Việc lấy trung bình cộng của lỗi này nhằm giúp tránh trường hợp hàm mất mát và đạo hàm có giá trị là một số rất lớn, ảnh hưởng tới độ chính xác của các phép toán khi thực hiện trên máy tính. Về mặt toán học, nghiệm của hai bài toán là như nhau.</p><p>Đạo hàm của hàm mất mát là:\n\\[\n\\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w}) = \n\\frac{1}{N}\\mathbf{\\bar{X}}^T \\mathbf{(\\bar{X}w - y)} ~~~~~(1)\n\\]</p><p><a name=\"sau-day-la-vi-du-tren-python-va-mot-vai-luu-y-khi-lap-trinh\"></a></p><h3 id=\"sau-đây-là-ví-dụ-trên-python-và-một-vài-lưu-ý-khi-lập-trình\">Sau đây là ví dụ trên Python và một vài lưu ý khi lập trình</h3><p>Load thư viện</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># To support both python 2 and python 3\n</span><span class=\"kn\">from</span> <span class=\"nn\">__future__</span> <span class=\"kn\">import</span> <span class=\"n\">division</span><span class=\"p\">,</span> <span class=\"n\">print_function</span><span class=\"p\">,</span> <span class=\"n\">unicode_literals</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span> \n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n</code></pre></div></div><p>Tiếp theo, chúng ta tạo 1000 điểm dữ liệu được chọn <em>gần</em> với đường thẳng \\(y = 4 + 3x\\), hiển thị chúng và tìm nghiệm theo công thức:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"mi\">4</span> <span class=\"o\">+</span> <span class=\"mi\">3</span> <span class=\"o\">*</span> <span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"p\">.</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"c1\"># noise added\n</span>\n<span class=\"c1\"># Building Xbar \n</span><span class=\"n\">one</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"n\">Xbar</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">concatenate</span><span class=\"p\">((</span><span class=\"n\">one</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Xbar</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">Xbar</span><span class=\"p\">)</span>\n<span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Xbar</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">w_lr</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">linalg</span><span class=\"p\">.</span><span class=\"n\">pinv</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">),</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Solution found by formula: w = '</span><span class=\"p\">,</span><span class=\"n\">w_lr</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Display result\n</span><span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">w_lr</span>\n<span class=\"n\">w_0</span> <span class=\"o\">=</span> <span class=\"n\">w</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">w_1</span> <span class=\"o\">=</span> <span class=\"n\">w</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">x0</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">endpoint</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"n\">y0</span> <span class=\"o\">=</span> <span class=\"n\">w_0</span> <span class=\"o\">+</span> <span class=\"n\">w_1</span><span class=\"o\">*</span><span class=\"n\">x0</span>\n\n<span class=\"c1\"># Draw the fitting line \n</span><span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"s\">'b.'</span><span class=\"p\">)</span>     <span class=\"c1\"># data \n</span><span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x0</span><span class=\"p\">,</span> <span class=\"n\">y0</span><span class=\"p\">,</span> <span class=\"s\">'y'</span><span class=\"p\">,</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">)</span>   <span class=\"c1\"># the fitting line\n</span><span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">axis</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div></div><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Solution found by formula: w =  [[ 4.00305242  2.99862665]]\n</code></pre></div></div><div class=\"imgcap\">\n<img align=\"center\" src=\"/assets/GD/output_11_1.png\" width=\"400\"/>\n</div><p>Đường thẳng tìm được là đường có màu vàng có phương trình \\(y \\approx 4 + 2.998x\\).</p><p>Tiếp theo ta viết đạo hàm và hàm mất mát:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">grad</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">):</span>\n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">Xbar</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"k\">return</span> <span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"n\">N</span> <span class=\"o\">*</span> <span class=\"n\">Xbar</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Xbar</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">cost</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">):</span>\n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">Xbar</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"k\">return</span> <span class=\"p\">.</span><span class=\"mi\">5</span><span class=\"o\">/</span><span class=\"n\">N</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">linalg</span><span class=\"p\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">y</span> <span class=\"o\">-</span> <span class=\"n\">Xbar</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">),</span> <span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">;</span>\n</code></pre></div></div><p><a name=\"kiem-tra-dao-ham\"></a></p><h4 id=\"kiểm-tra-đạo-hàm\">Kiểm tra đạo hàm</h4><p>Việc tính đạo hàm của hàm nhiều biến thông thường khá phức tạp và rất dễ mắc lỗi, nếu chúng ta tính sai đạo hàm thì thuật toán GD không thể chạy đúng được. Trong thực nghiệm, có một cách để kiểm tra liệu đạo hàm tính được có chính xác không. Cách này dựa trên định nghĩa của đạo hàm (cho hàm 1 biến):\n\\[\nf’(x) = \\lim_{\\varepsilon \\rightarrow 0}\\frac{f(x + \\varepsilon) - f(x)}{\\varepsilon}\n\\]</p><p>Một cách thường được sử dụng là lấy một giá trị \\(\\varepsilon \\) rất nhỏ, ví dụ \\(10^{-6}\\), và sử dụng công thức:\n\\[\nf’(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2\\varepsilon} ~~~~ (2)\n\\]</p><p>Cách tính này được gọi là <em>numerical gradient</em>.</p><p><strong>Câu hỏi: Tại sao công thức xấp xỉ hai phía trên đây lại được sử dụng rộng rãi, sao không sử dụng công thức xấp xỉ đạo hàm bên phải hoặc bên trái?</strong></p><p>Có hai các giải thích cho vấn đề này, một bằng hình học, một bằng giải tích.</p><p><a name=\"giai-thich-bang-hinh-hoc\"></a></p><h5 id=\"giải-thích-bằng-hình-học\">Giải thích bằng hình học</h5><p>Quan sát hình dưới đây:</p><div class=\"imgcap\">\n<img align=\"center\" src=\"/assets/GD/check_grad.png\" width=\"600\"/>\n</div><p><br>\nTrong hình, vector màu đỏ là đạo hàm <em>chính xác</em> của hàm số tại điểm có hoành độ bằng \\(x_0\\). Vector màu xanh lam (có vẻ là hơi tím sau khi convert từ .pdf sang .png) thể hiện cách xấp xỉ đạo hàm phía phải. Vector màu xanh lục thể hiện cách xấp xỉ đạo hàm phía trái. Vector màu nâu thể hiện cách xấp xỉ đạo hàm hai phía. Trong ba vector xấp xỉ đó, vector xấp xỉ hai phía màu nâu là gần với vector đỏ nhất nếu xét theo hướng.</br></p><p>Sự khác biệt giữa các cách xấp xỉ còn lớn hơn nữa nếu tại điểm x, hàm số bị <em>bẻ cong</em> mạnh hơn. Khi đó, xấp xỉ trái và phải sẽ khác nhau rất nhiều. Xấp xỉ hai bên sẽ <em>ổn định</em> hơn.</p><p><a name=\"giai-thich-bang-giai-tich\"></a></p><h5 id=\"giải-thích-bằng-giải-tích\">Giải thích bằng giải tích</h5><p>Chúng ta cùng quay lại một chút với Giải tích I năm thứ nhất đại học: <a href=\"http://mathworld.wolfram.com/TaylorSeries.html\">Khai triển Taylor</a>.</p><p>Với \\(\\varepsilon\\) rất nhỏ, ta có hai xấp xỉ sau:</p><p>\\[\nf(x + \\varepsilon) \\approx f(x) + f’(x)\\varepsilon + \\frac{f”(x)}{2} \\varepsilon^2 + \\dots\n\\]</p><p>và:\n\\[\nf(x - \\varepsilon) \\approx f(x) - f’(x)\\varepsilon + \\frac{f”(x)}{2} \\varepsilon^2 - \\dots\n\\]</p><p>Từ đó ta có: \n\\[\n\\frac{f(x + \\varepsilon) - f(x)}{\\varepsilon} \\approx f’(x) + \\frac{f”(x)}{2}\\varepsilon + \\dots =  f’(x) + O(\\varepsilon) ~~ (3)\n\\]</p><p>\\[\n\\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2\\varepsilon} \\approx f’(x) + \\frac{f^{(3)}(x)}{6}\\varepsilon^2 + \\dots =  f’(x) + O(\\varepsilon^2) ~~(4)\n\\]</p><p>Từ đó, nếu xấp xỉ đạo hàm bằng công thức \\((3)\\) (xấp xỉ đạo hàm phải), sai số sẽ là \\(O(\\varepsilon)\\). Trong khi đó, nếu xấp xỉ đạo hàm bằng công thức \\((4)\\) (xấp xỉ đạo hàm hai phía), sai số sẽ là \\(O(\\varepsilon^2) \\ll O(\\varepsilon)\\) nếu \\(\\varepsilon\\) nhỏ.</p><p>Cả hai cách giải thích trên đây đều cho chúng ta thấy rằng, xấp xỉ đạo hàm hai\nphía là xấp xỉ tốt hơn.</p><p><a name=\"voi-ham-nhieu-bien\"></a></p><h5 id=\"với-hàm-nhiều-biến\">Với hàm nhiều biến</h5><p>Với hàm nhiều biến, công thức \\((2)\\) được áp dụng cho từng biến khi các biến\nkhác cố định. Cách tính này thường cho giá trị khá chính xác. Tuy nhiên, cách\nnày không được sử dụng để tính đạo hàm vì độ phức tạp quá cao so với cách tính\ntrực tiếp. Khi so sánh đạo hàm này với đạo hàm chính xác tính theo công thức,\nngười ta thường giảm số chiều dữ liệu và giảm số điểm dữ liệu để thuận tiện cho\ntính toán. Một khi đạo hàm tính được rất gần với <em>numerical gradient</em>, chúng ta\ncó thể tự tin rằng đạo hàm tính được là chính xác.</p><p>Dưới đây là một đoạn code đơn giản để kiểm tra đạo hàm và có thể áp dụng với một\nhàm số (của một vector) bất kỳ với <code class=\"language-plaintext highlighter-rouge\">cost</code> và <code class=\"language-plaintext highlighter-rouge\">grad</code> đã tính ở phía trên.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">numerical_grad</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">):</span>\n    <span class=\"n\">eps</span> <span class=\"o\">=</span> <span class=\"mf\">1e-4</span>\n    <span class=\"n\">g</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">)):</span>\n        <span class=\"n\">w_p</span> <span class=\"o\">=</span> <span class=\"n\">w</span><span class=\"p\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n        <span class=\"n\">w_n</span> <span class=\"o\">=</span> <span class=\"n\">w</span><span class=\"p\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n        <span class=\"n\">w_p</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">eps</span> \n        <span class=\"n\">w_n</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">-=</span> <span class=\"n\">eps</span>\n        <span class=\"n\">g</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">(</span><span class=\"n\">w_p</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">cost</span><span class=\"p\">(</span><span class=\"n\">w_n</span><span class=\"p\">))</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">eps</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">g</span> \n\n<span class=\"k\">def</span> <span class=\"nf\">check_grad</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">):</span>\n    <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">w</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n    <span class=\"n\">grad1</span> <span class=\"o\">=</span> <span class=\"n\">grad</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">)</span>\n    <span class=\"n\">grad2</span> <span class=\"o\">=</span> <span class=\"n\">numerical_grad</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"bp\">True</span> <span class=\"k\">if</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">linalg</span><span class=\"p\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grad1</span> <span class=\"o\">-</span> <span class=\"n\">grad2</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"mf\">1e-6</span> <span class=\"k\">else</span> <span class=\"bp\">False</span> \n\n<span class=\"k\">print</span><span class=\"p\">(</span> <span class=\"s\">'Checking gradient...'</span><span class=\"p\">,</span> <span class=\"n\">check_grad</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">cost</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">))</span>\n</code></pre></div></div><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Checking gradient... True\n</code></pre></div></div><p>(<em>Với các hàm số khác, bạn đọc chỉ cần viết lại hàm <code class=\"language-plaintext highlighter-rouge\">grad</code> và <code class=\"language-plaintext highlighter-rouge\">cost</code> ở phần trên\nrồi áp dụng đoạn code này để kiểm tra đạo hàm. Nếu hàm số là hàm của một ma trận\nthì chúng ta thay đổi một chút trong hàm <code class=\"language-plaintext highlighter-rouge\">numerical_grad</code>, tôi hy vọng không quá\nphức tạp</em>).</p><p>Với bài toán Linear Regression, cách tính đạo hàm như trong \\((1)\\) phía trên\nđược coi là đúng vì sai số giữa hai cách tính là rất nhỏ (nhỏ hơn\n\\(10^{-6}\\)). Sau khi có được đạo hàm chính xác, chúng ta viết hàm cho GD:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">myGD</span><span class=\"p\">(</span><span class=\"n\">w_init</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">,</span> <span class=\"n\">eta</span><span class=\"p\">):</span>\n    <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w_init</span><span class=\"p\">]</span>\n    <span class=\"k\">for</span> <span class=\"n\">it</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n        <span class=\"n\">w_new</span> <span class=\"o\">=</span> <span class=\"n\">w</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">eta</span><span class=\"o\">*</span><span class=\"n\">grad</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n        <span class=\"k\">if</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">linalg</span><span class=\"p\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grad</span><span class=\"p\">(</span><span class=\"n\">w_new</span><span class=\"p\">))</span><span class=\"o\">/</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">w_new</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"mf\">1e-3</span><span class=\"p\">:</span>\n            <span class=\"k\">break</span> \n        <span class=\"n\">w</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">w_new</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">it</span><span class=\"p\">)</span> \n\n<span class=\"n\">w_init</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]])</span>\n<span class=\"p\">(</span><span class=\"n\">w1</span><span class=\"p\">,</span> <span class=\"n\">it1</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">myGD</span><span class=\"p\">(</span><span class=\"n\">w_init</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'Solution found by GD: w = '</span><span class=\"p\">,</span> <span class=\"n\">w1</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">].</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"s\">',</span><span class=\"se\">\\n</span><span class=\"s\">after %d iterations.'</span> <span class=\"o\">%</span><span class=\"p\">(</span><span class=\"n\">it1</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n</code></pre></div></div><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Solution found by GD: w =  [[ 4.01780793  2.97133693]] ,\nafter 49 iterations.\n</code></pre></div></div><p>Sau 49 vòng lặp, thuật toán đã hội tụ với một nghiệm khá gần với nghiệm tìm được\ntheo công thức.</p><p>Dưới đây là hình động minh họa thuật toán GD.</p><table style=\"border: 0px solid white\" width=\"100%\">\n<tr>\n<td style=\"border: 0px solid white\" width=\"40%\">\n<img src=\"/assets/GD/img1_1.gif\" style=\"display:block;\" width=\"100%\"/>\n</td>\n<td style=\"border: 0px solid white\" width=\"40%\">\n<img src=\"/assets/GD/img2_1.gif\" style=\"display:block;\" width=\"100%\"/>\n</td>\n</tr>\n</table><p>Trong hình bên trái, các đường thẳng màu đỏ là nghiệm tìm được sau mỗi vòng lặp.</p><p>Trong hình bên phải, tôi xin giới thiệu một thuật ngữ mới: <em>đường đồng mức</em>.\n<a name=\"duong-dong-muc-level-sets\"></a></p><h4 id=\"đường-đồng-mức-level-sets\">Đường đồng mức (level sets)</h4><p>Với đồ thị của một hàm số với hai biến đầu vào cần được vẽ trong không gian ba\nchiều, nhều khi chúng ta khó nhìn được nghiệm có khoảng tọa độ bao nhiêu. Trong\ntoán tối ưu, người ta thường dùng một cách vẽ sử dụng khái niệm <em>đường đồng mức</em>\n(level sets).</p><p>Nếu các bạn để ý trong các bản độ tự nhiên, để miêu tả độ cao của các dãy núi,\nngười ta dùng nhiều đường cong kín bao quanh nhau như sau:</p><div class=\"imgcap\">\n<img align=\"center\" src=\"http://files.vforum.vn/2016/T06/img/vforum.vn-324944-hinh-44-lc6b0e1bba3c-c491e1bb93-c491e1bb8ba-hc3acnh-te1bb89-le1bb87-le1bb9bn.png\" width=\"600\"/>\n<div class=\"thecap\"> Ví dụ về đường đồng mức trong các bản đồ tự nhiên. (Nguồn: <a href=\"http://vforum.vn/diendan/showthread.php?90166-Dia-ly-6-Duong-dong-muc-la-nhung-duong-nhu-the-nao-\">Địa lý 6: Đường đồng mức là những đường như thế nào?</a>)</div>\n</div><p>Các vòng nhỏ màu đỏ hơn thể hiện các điểm ở trên cao hơn.</p><p>Trong toán tối ưu, người ta cũng dùng phương pháp này để thể hiện các bề mặt\ntrong không gian hai chiều.</p><p>Quay trở lại với hình minh họa thuật toán GD cho bài toán Liner Regression bên\ntrên, hình bên phải là hình biểu diễn các level sets. Tức là tại các điểm trên\ncùng một vòng, hàm mất mát có giá trị như nhau. Trong ví dụ này, tôi hiển thị\ngiá trị của hàm số tại một số vòng. Các vòng màu xanh có giá trị thấp, các vòng\ntròn màu đỏ phía ngoài có giá trị cao hơn. Điểm này khác một chút so với đường\nđồng mức trong tự nhiên là các vòng bên trong thường thể hiện một thung lũng hơn\nlà một đỉnh núi (vì chúng ta đang đi tìm giá trị nhỏ nhất).</p><p>Tôi thử với <em>learning rate</em> nhỏ hơn, kết quả như sau:</p><table style=\"border: 0px solid white\" width=\"100%\">\n<tr>\n<td style=\"border: 0px solid white\" width=\"40%\">\n<img src=\"/assets/GD/img1_0.1.gif\" style=\"display:block;\" width=\"100%\"/>\n</td>\n<td style=\"border: 0px solid white\" width=\"40%\">\n<img src=\"/assets/GD/img2_0.1.gif\" style=\"display:block;\" width=\"100%\"/>\n</td>\n</tr>\n</table><p>Tốc độ hội tụ đã chậm đi nhiều, thậm chí sau 99 vòng lặp, GD vẫn chưa tới gần\nđược nghiệm tốt nhất. Trong các bài toán thực tế, chúng ta cần nhiều vòng lặp\nhơn 99 rất nhiều, vì số chiều và số điểm dữ liệu thường là rất lớn.</p><p><a name=\"-mot-vi-du-khac\"></a></p>"}}