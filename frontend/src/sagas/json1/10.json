{"topic": "<ul class=\"tags\">\n<a class=\"tag\" href=\"/tags#Neural-nets\">Neural-nets</a>\n<a class=\"tag\" href=\"/tags#Supervised-learning\">Supervised-learning</a>\n<a class=\"tag\" href=\"/tags#Classification\">Classification</a>\n<a class=\"tag\" href=\"/tags#Linear-models\">Linear-models</a>\n<a class=\"tag\" href=\"/tags#GD\">GD</a>\n</ul>", "title": "<h1 class=\"post-title\" itemprop=\"name\">Bài 9: Perceptron Learning Algorithm</h1>", "introduction": {"title": "<h2 id=\"1-giới-thiệu\">1. Giới thiệu</h2>", "content": "<p>Trong bài này, tôi sẽ giới thiệu thuật toán đầu tiên trong Classification có tên là Perceptron Learning Algorithm (PLA) hoặc đôi khi được viết gọn là Perceptron.</p><p>Perceptron là một thuật toán Classification cho trường hợp đơn giản nhất: chỉ có hai class (lớp) (<em>bài toán với chỉ hai class được gọi là binary classification</em>) và cũng chỉ hoạt động được trong một trường hợp rất cụ thể. Tuy nhiên, nó là nền tảng cho một mảng lớn quan trọng của Machine Learning là Neural Networks và sau này là Deep Learning. (Tại sao lại gọi là Neural Networks - tức mạng dây thần kinh - các bạn sẽ được thấy ở cuối bài).</p><p>Giả sử chúng ta có hai tập hợp dữ liệu đã được gán nhãn được minh hoạ trong Hình 1 bên trái dưới đây. Hai class của chúng ta là tập các điểm màu xanh và tập các điểm màu đỏ. Bài toán đặt ra là: từ dữ liệu của hai tập được gán nhãn cho trước, hãy xây dựng một <em>classifier</em> (bộ phân lớp) để khi có một điểm dữ liệu hình tam giác màu xám mới, ta có thể dự đoán được màu (nhãn) của nó.</p><table style=\"border: 0px solid white\" width=\"100%\">\n<tr>\n<td style=\"border: 0px solid white\" width=\"40%\">\n<img src=\"/assets/pla/pla1.png\" style=\"display:block;\" width=\"100%\"/>\n</td>\n<td style=\"border: 0px solid white\" width=\"40%\">\n<img src=\"/assets/pla/pla2.png\" style=\"display:block;\" width=\"100%\"/>\n</td>\n</tr>\n</table><div class=\"thecap\">Hình 1: Bài toán Perceptron</div><p>Hiểu theo một cách khác, chúng ta cần tìm <em>lãnh thổ</em> của mỗi class sao cho, với mỗi một điểm mới, ta chỉ cần xác định xem nó nằm vào lãnh thổ của class nào rồi quyết định nó thuộc class đó. Để tìm <em>lãnh thổ</em> của mỗi class, chúng ta cần đi tìm biên giới (boundary) giữa hai <em>lãnh thổ</em> này. Vậy bài toán classification có thể coi là bài toán đi tìm boundary giữa các class. Và boundary đơn giản nhất trong không gian hai chiều là một đường thằng, trong không gian ba chiều là một mặt phẳng, trong không gian nhiều chiều là một siêu mặt phẳng (hyperplane) (tôi gọi chung những boundary này là <em>đường phẳng</em>). Những boundary phẳng này được coi là đơn giản vì nó có thể biểu diễn dưới dạng toán học bằng một hàm số đơn giản có dạng tuyến tính, tức linear. Tất nhiên, chúng ta đang giả sử rằng tồn tại một đường phẳng để có thể phân định <em>lãnh thổ</em> của hai class. Hình 1 bên phải minh họa một đường thẳng phân chia hai class trong mặt phẳng. Phần có nền màu xanh được coi là <em>lãnh thổ</em> của lớp xanh, phần có nên màu đỏ được coi là <em>lãnh thổ</em> của lớp đỏ. Trong trường hợp này, điểm dữ liệu mới hình tam giác được phân vào class đỏ.</p><p><a name=\"bai-toan-perceptron\"></a></p><h3 id=\"bài-toán-perceptron\">Bài toán Perceptron</h3><p>Bài toán Perceptron được phát biểu như sau: <em>Cho hai class được gán nhãn, hãy tìm một đường phẳng sao cho toàn bộ các điểm thuộc class 1 nằm về 1 phía, toàn bộ các điểm thuộc class 2 nằm về phía còn lại của đường phẳng đó. Với giả định rằng tồn tại một đường phẳng như thế.</em></p><p>Nếu tồn tại một đường phẳng phân chia hai class thì ta gọi hai class đó là <em>linearly separable</em>. Các thuật toán classification tạo ra các boundary là các đường phẳng được gọi chung là Linear Classifier.</p><p><a name=\"-thuat-toan-perceptron-pla\"></a></p>"}, "formulas": {"title": "<h2 id=\"2-thuật-toán-perceptron-pla\">2. Thuật toán Perceptron (PLA)</h2>", "content": "<p>Cũng giống như các thuật toán lặp trong <a href=\"/2017/01/01/kmeans/\">K-means Clustering</a> và <a href=\"/2017/01/12/gradientdescent/\">Gradient Descent</a>, ý tưởng cơ bản của PLA là xuất phát từ một nghiệm dự đoán nào đó, qua mỗi vòng lặp, nghiệm sẽ được cập nhật tới một ví trí tốt hơn. Việc cập nhật này dựa trên việc giảm giá trị của một hàm mất mát nào đó.</p><p><a name=\"mot-so-ky-hieu\"></a></p><h3 id=\"một-số-ký-hiệu\">Một số ký hiệu</h3><p>Giả sử \\(\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N] \\in \\mathbb{R}^{d \\times N}\\) là ma trận chứa các điểm dữ liệu mà mỗi cột \\(\\mathbf{x}_i \\in \\mathbb{R}^{d\\times 1}\\) là một điểm dữ liệu trong không gian \\(d\\) chiều. (<em>Chú ý: khác với các bài trước tôi thường dùng các vector hàng để mô tả dữ liệu, trong bài này tôi dùng vector cột để biểu diễn. Việc biểu diễn dữ liệu ở dạng hàng hay cột tùy thuộc vào từng bài toán, miễn sao cách biểu diễn toán học của nó khiến cho người đọc thấy dễ hiểu</em>).</p><p>Giả sử thêm các nhãn tương ứng với từng điểm dữ liệu được lưu trong một vector hàng \\(\\mathbf{y} = [y_1, y_2, \\dots, y_N] \\in \\mathbb{R}^{1\\times N}\\), với \\(y_i = 1\\) nếu \\(\\mathbf{x}_i\\) thuộc class 1 (xanh) và \\(y_i = -1\\) nếu \\(\\mathbf{x}_i\\) thuộc class 2 (đỏ).</p><p>Tại một thời điểm, giả sử ta tìm được boundary là đường phẳng có phương trình:\n\\[\n\\begin{eqnarray}\nf_{\\mathbf{w}}(\\mathbf{x}) &amp;=&amp; w_1x_1 + \\dots + w_dx_d + w_0 \\newline \n&amp;=&amp;\\mathbf{w}^T\\mathbf{\\bar{x}} = 0\n\\end{eqnarray}\n\\]</p><p>với \\(\\mathbf{\\bar{x}}\\) là điểm dữ liệu mở rộng bằng cách thêm phần tử \\(x_0 = 1\\) lên trước vector \\(\\mathbf{x}\\) tương tự như trong <a href=\"/2016/12/28/linearregression/\">Linear Regression</a>. Và từ đây, khi nói \\(\\mathbf{x}\\), tôi cũng ngầm hiểu là điểm dữ liệu mở rộng.</p><p>Để cho đơn giản, chúng ta hãy cùng làm việc với trường hợp mỗi điểm dữ liệu có số chiều \\(d = 2\\). Giả sử đường thẳng \\(w_1 x_1 + w_2 x_2 + w_0 = 0\\) chính là nghiệm cần tìm như Hình 2 dưới đây:</p><div class=\"imgcap\">\n<img align=\"center\" src=\"\\assets\\pla\\pla4.png\" width=\"400\"/>\n<div class=\"thecap\">Hình 2: Phương trình đường thẳng boundary.</div>\n</div><p>Nhận xét rằng các điểm nằm về cùng 1 phía so với đường thẳng này sẽ làm cho hàm số \\(f_{\\mathbf{w}}(\\mathbf{x})\\) mang cùng dấu. Chỉ cần đổi dấu của \\(\\mathbf{w}\\) nếu cần thiết, ta có thể giả sử các điểm nằm trong nửa mặt phẳng nền xanh mang dấu dương (+), các điểm nằm trong nửa mặt phẳng nền đỏ mang dấu âm (-). Các dấu này cũng tương đương với nhãn \\(y\\) của mỗi class. Vậy nếu \\(\\mathbf{w}\\) là một nghiệm của bài toán Perceptron, với một điểm dữ liệu mới \\(\\mathbf{x}\\) chưa được gán nhãn, ta có thể xác định class của nó bằng phép toán đơn giản như sau:\n\\[\n\\text{label}(\\mathbf{x}) = 1 ~\\text{if}~ \\mathbf{w}^T\\mathbf{x} \\geq 0, \\text{otherwise} -1\n\\]</p><p>Ngắn gọn hơn: \n\\[\n\\text{label}(\\mathbf{x}) = \\text{sgn}(\\mathbf{w}^T\\mathbf{x})\n\\]\ntrong đó, \\(\\text{sgn}\\) là hàm xác định dấu, với giả sử rằng \\(\\text{sgn}(0) = 1\\).</p><p><a name=\"xay-dung-ham-mat-mat\"></a></p><h3 id=\"xây-dựng-hàm-mất-mát\">Xây dựng hàm mất mát</h3><p>Tiếp theo, chúng ta cần xây dựng hàm mất mát với tham số \\(\\mathbf{w}\\) bất kỳ. Vẫn trong không gian hai chiều, giả sử đường thẳng \\(w_1x_1 + w_2x_2 + w_0 = 0\\) được cho như Hình 3 dưới đây:</p><div class=\"imgcap\">\n<img align=\"center\" src=\"\\assets\\pla\\pla3.png\" width=\"400\"/>\n<div class=\"thecap\">Hình 3: Đường thẳng bất kỳ và các điểm bị misclassified được khoanh tròn.</div>\n</div><p>Trong trường hợp này, các điểm được khoanh tròn là các điểm bị misclassified (phân lớp lỗi). Điều chúng ta mong muốn là không có điểm nào bị misclassified. Hàm mất mát đơn giản nhất chúng ta nghĩ đến là hàm <em>đếm</em> số lượng các điểm bị misclassied và tìm cách tối thiểu hàm số này:\n\\[\nJ_1(\\mathbf{w}) = \\sum_{\\mathbf{x}_i \\in \\mathcal{M}} (-y_i\\text{sgn}(\\mathbf{w}^T\\mathbf{x_i}))\n\\]</p><p>trong đó \\(\\mathcal{M}\\) là tập hợp các điểm bị misclassifed (<em>tập hợp này thay đổi theo</em> \\(\\mathbf{w}\\)). Với mỗi điểm \\(\\mathbf{x}_i \\in \\mathcal{M}\\), vì điểm này bị misclassified nên \\(y_i\\) và \\(\\text{sgn}(\\mathbf{w}^T\\mathbf{x})\\) khác nhau, và vì thế \\(-y_i\\text{sgn}(\\mathbf{w}^T\\mathbf{x_i}) = 1 \\). Vậy \\(J_1(\\mathbf{w})\\) chính là hàm <em>đếm</em> số lượng các điểm bị misclassified. Khi hàm số này đạt giá trị nhỏ nhất bằng 0 thì ta không còn điểm nào bị misclassified.</p><p>Một điểm quan trọng, hàm số này là rời rạc, không tính được đạo hàm theo \\(\\mathbf{w}\\) nên rất khó tối ưu. Chúng ta cần tìm một hàm mất mát khác để việc tối ưu khả thi hơn.</p><p>Xét hàm mất mát sau đây:</p><p>\\[\nJ(\\mathbf{w}) = \\sum_{\\mathbf{x}_i \\in \\mathcal{M}} (-y_i\\mathbf{w}^T\\mathbf{x_i})\n\\]</p><p>Hàm \\(J()\\) khác một chút với hàm \\(J_1()\\) ở việc bỏ đi hàm \\(\\text{sgn}\\). Nhận xét rằng khi một điểm misclassified \\(\\mathbf{x}_i\\) nằm càng xa boundary thì giá trị \\(-y_i\\mathbf{w}^T\\mathbf{x}_i\\) sẽ càng lớn, nghĩa là sự sai lệch càng lớn. Giá trị nhỏ nhất của hàm mất mát này cũng bằng 0 nếu không có điểm nào bị misclassifed. Hàm mất mát này cũng được cho là tốt hơn hàm \\(J_1()\\) vì nó <em>trừng phạt</em> rất nặng những điểm <em>lấn sâu sang lãnh thổ của class kia</em>. Trong khi đó, \\(J_1()\\) <em>trừng phạt</em> các điểm misclassified như nhau (đều = 1), bất kể chúng xa hay gần với đường biên giới.</p><p>Tại một thời điểm, nếu chúng ta chỉ quan tâm tới các điểm bị misclassified thì hàm số \\(J(\\mathbf{w})\\) khả vi (tính được đạo hàm), vậy chúng ta có thể sử dụng <a href=\"/2017/01/12/gradientdescent/\">Gradient Descent</a> hoặc <a href=\"/2017/01/16/gradientdescent2/#-stochastic-gradient-descent\">Stochastic Gradient Descent (SGD)</a> để tối ưu hàm mất mát này. Với ưu điểm của SGD cho các bài toán <a href=\"/2017/01/12/gradientdescent/#large-scale\">large-scale</a>, chúng ta sẽ làm theo thuật toán này.</p><p>Với <em>một</em> điểm dữ liệu \\(\\mathbf{x}_i\\) bị misclassified, hàm mất mát trở thành:</p><p>\\[\nJ(\\mathbf{w}; \\mathbf{x}_i; y_i) = -y_i\\mathbf{w}^T\\mathbf{x}_i\n\\]</p><p>Đạo hàm tương ứng:</p><p>\\[\n\\nabla_{\\mathbf{w}}J(\\mathbf{w}; \\mathbf{x}_i; y_i) = -y_i\\mathbf{x}_i\n\\]</p><p>Vậy quy tắc cập nhật là:</p><p>\\[\n\\mathbf{w} = \\mathbf{w} + \\eta y_i\\mathbf{x}_i\n\\]</p><p>với \\(\\eta\\) là learning rate được chọn bằng 1. Ta có một quy tắc cập nhật rất gọn là: \\(\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + y_i\\mathbf{x}_i\\). Nói cách khác, với mỗi điểm \\(\\mathbf{x}_i\\) bị misclassifed, ta chỉ cần nhân điểm đó với nhãn \\(y_i\\) của nó, lấy kết quả cộng vào \\(\\mathbf{w}\\) ta sẽ được \\(\\mathbf{w}\\) mới.</p><p>Ta có một quan sát nhỏ ở đây:\n\\[\n\\mathbf{w}_{t+1}^T\\mathbf{x}_i = (\\mathbf{w}_{t} + y_i\\mathbf{x}_i)^T\\mathbf{x}_{i} \\newline\n= \\mathbf{w}_{t}^T\\mathbf{x}_i + y_i ||\\mathbf{x}_i||_2^2\n\\]</p><p>Nếu \\(y_i = 1\\), vì \\(\\mathbf{x}_i\\) bị misclassifed nên \\(\\mathbf{w}_{t}^T\\mathbf{x}_i &lt; 0\\). Cũng vì \\(y_i = 1\\) nên \\(y_i ||\\mathbf{x}_i||_2^2 = ||\\mathbf{x}_i||_2^2 \\geq 1\\) (chú ý \\(x_0 = 1\\)), nghĩa là \\(\\mathbf{w}_{t+1}^T\\mathbf{x}_i &gt; \\mathbf{w}_{t}^T\\mathbf{x}_i\\). Lý giải bằng lời, \\(\\mathbf{w}_{t+1}\\) tiến về phía làm cho \\(\\mathbf{x}_i\\) được phân lớp đúng. Điều tương tự xảy ra nếu \\(y_i = -1\\).</p><p>Đến đây, cảm nhận của chúng ta với thuật toán này là: cứ chọn đường boundary đi. Xét từng điểm một, nếu điểm đó bị misclassified thì tiến đường boundary về phía làm cho điểm đó được classifed đúng. Có thể thấy rằng, khi di chuyển đường boundary này, các điểm trước đó được classified đúng có thể lại bị misclassified. Mặc dù vậy, PLA vẫn được đảm bảo sẽ hội tụ sau một số hữu hạn bước (tôi sẽ chứng minh việc này ở phía sau của bài viết). Tức là cuối cùng, ta sẽ tìm được đường phẳng phân chia hai lớp, miễn là hai lớp đó là linearly separable. Đây cũng chính là lý do câu đầu tiên trong bài này tôi nói với các bạn là: “Cứ làm đi, sai đâu sửa đấy, cuối cùng sẽ thành công!”.</p><p>Tóm lại, thuật toán Perceptron có thể được viết như sau:</p><p><a name=\"tom-tat-pla\"></a></p><h3 id=\"tóm-tắt-pla\">Tóm tắt PLA</h3><ol>\n<li>Chọn ngẫu nhiên một vector hệ số \\(\\mathbf{w}\\) với các phần tử gần 0.</li>\n<li>Duyệt ngẫu nhiên qua từng điểm dữ liệu \\(\\mathbf{x}_i\\):\n    <ul>\n<li>Nếu \\(\\mathbf{x}_i\\) được phân lớp đúng, tức \\(\\text{sgn}(\\mathbf{w}^T\\mathbf{x}_i) = y_i\\), chúng ta không cần làm gì.</li>\n<li>Nếu \\(\\mathbf{x}_i\\) bị misclassifed, cập nhật \\(\\mathbf{w}\\) theo công thức:\n \\[\n \\mathbf{w} = \\mathbf{w} + y_i\\mathbf{x}_i\n \\]</li>\n</ul>\n</li>\n<li>Kiểm tra xem có bao nhiêu điểm bị misclassifed. Nếu không còn điểm nào, dừng thuật toán. Nếu còn, quay lại bước 2.</li>\n</ol><p><a name=\"-vi-du-tren-python\"></a></p>"}, "examples": {"title": "<h2 id=\"3-ví-dụ-trên-python\">3. Ví dụ trên Python</h2>", "content": "<p>Như thường lệ, chúng ta sẽ thử một ví dụ nhỏ với Python.</p><p><a name=\"load-thu-vien-va-tao-du-lieu\"></a></p><h3 id=\"load-thư-viện-và-tạo-dữ-liệu\">Load thư viện và tạo dữ liệu</h3><p>Chúng ta sẽ tạo hai nhóm dữ liệu, mỗi nhóm có 10 điểm, mỗi điểm dữ liệu có hai chiều để thuận tiện cho việc minh họa. Sau đó, tạo dữ liệu mở rộng bằng cách thêm 1 vào đầu mỗi điểm dữ liệu.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># generate data\n# list of points \n</span><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span> \n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">from</span> <span class=\"nn\">scipy.spatial.distance</span> <span class=\"kn\">import</span> <span class=\"n\">cdist</span>\n<span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"n\">means</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">]]</span>\n<span class=\"n\">cov</span> <span class=\"o\">=</span> <span class=\"p\">[[.</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"p\">.</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[.</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"p\">.</span><span class=\"mi\">3</span><span class=\"p\">]]</span>\n<span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"n\">X0</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">multivariate_normal</span><span class=\"p\">(</span><span class=\"n\">means</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">cov</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">).</span><span class=\"n\">T</span>\n<span class=\"n\">X1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">multivariate_normal</span><span class=\"p\">(</span><span class=\"n\">means</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">cov</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">).</span><span class=\"n\">T</span>\n\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">concatenate</span><span class=\"p\">((</span><span class=\"n\">X0</span><span class=\"p\">,</span> <span class=\"n\">X1</span><span class=\"p\">),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">concatenate</span><span class=\"p\">((</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)),</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">))),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"c1\"># Xbar \n</span><span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">concatenate</span><span class=\"p\">((</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">N</span><span class=\"p\">)),</span> <span class=\"n\">X</span><span class=\"p\">),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n</code></pre></div></div><p>Sau khi thực hiện đoạn code này, biến <code class=\"language-plaintext highlighter-rouge\">X</code> sẽ chứa dữ liệu input (mở rộng), biến <code class=\"language-plaintext highlighter-rouge\">y</code> sẽ chứa nhãn của mỗi điểm dữ liệu trong <code class=\"language-plaintext highlighter-rouge\">X</code>.\n<a name=\"cac-ham-so-cho-pla\"></a></p><h3 id=\"các-hàm-số-cho-pla\">Các hàm số cho PLA</h3><p>Tiếp theo chúng ta cần viết 3 hàm số cho PLA:</p><ol>\n<li><code class=\"language-plaintext highlighter-rouge\">h(w, x)</code>: tính đầu ra khi biết đầu vào <code class=\"language-plaintext highlighter-rouge\">x</code> và weights <code class=\"language-plaintext highlighter-rouge\">w</code>.</li>\n<li><code class=\"language-plaintext highlighter-rouge\">has_converged(X, y, w)</code>: kiểm tra xem thuật toán đã hội tụ chưa. Ta chỉ cần so sánh <code class=\"language-plaintext highlighter-rouge\">h(w, X)</code> với <em>ground truth</em> <code class=\"language-plaintext highlighter-rouge\">y</code>. Nếu giống nhau thì dừng thuật toán.</li>\n<li><code class=\"language-plaintext highlighter-rouge\">perceptron(X, y, w_init)</code>: hàm chính thực hiện PLA.</li>\n</ol><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">h</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>    \n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">sign</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">))</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">has_converged</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">):</span>    \n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array_equal</span><span class=\"p\">(</span><span class=\"n\">h</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">),</span> <span class=\"n\">y</span><span class=\"p\">)</span> \n\n<span class=\"k\">def</span> <span class=\"nf\">perceptron</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">w_init</span><span class=\"p\">):</span>\n    <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w_init</span><span class=\"p\">]</span>\n    <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">mis_points</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">while</span> <span class=\"bp\">True</span><span class=\"p\">:</span>\n        <span class=\"c1\"># mix data \n</span>        <span class=\"n\">mix_id</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">permutation</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">):</span>\n            <span class=\"n\">xi</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"n\">mix_id</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]].</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n            <span class=\"n\">yi</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">mix_id</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]]</span>\n            <span class=\"k\">if</span> <span class=\"n\">h</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">xi</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">!=</span> <span class=\"n\">yi</span><span class=\"p\">:</span> <span class=\"c1\"># misclassified point\n</span>                <span class=\"n\">mis_points</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">mix_id</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span>\n                <span class=\"n\">w_new</span> <span class=\"o\">=</span> <span class=\"n\">w</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">yi</span><span class=\"o\">*</span><span class=\"n\">xi</span> \n                <span class=\"n\">w</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">w_new</span><span class=\"p\">)</span>\n                \n        <span class=\"k\">if</span> <span class=\"n\">has_converged</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]):</span>\n            <span class=\"k\">break</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">mis_points</span><span class=\"p\">)</span>\n\n<span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">w_init</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">perceptron</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">w_init</span><span class=\"p\">)</span>\n</code></pre></div></div><p>Dưới đây là hình minh họa thuật toán PLA cho bài toán nhỏ này:</p><div class=\"imgcap\">\n<img align=\"center\" src=\"\\assets\\pla\\pla_vis.gif\" width=\"400\"/>\n<div class=\"thecap\"> Hình 4: Minh họa thuật toán PLA </div>\n</div><p>Sau khi cập nhật 18 lần, PLA đã hội tụ. Điểm được khoanh tròn màu đen là điểm misclassified tương ứng được chọn để cập nhật đường boundary.</p><p>Source code cho phần này (bao gồm hình động) <a href=\"https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/pla/perceptron.py\">có thể được tìm thấy ở đây</a>.</p><p><a name=\"-chung-minh-hoi-tu\"></a></p>"}}