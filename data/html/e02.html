<!DOCTYPE html>
<html>
<head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#">
<meta charset="utf-8" />
<meta http-equiv='X-UA-Compatible' content='IE=edge'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
<title>Machine Learning cơ bản</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300" rel="stylesheet">
  <!-- <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"> -->
  <link href="https://fonts.googleapis.com/css?family=Roboto|Source+Sans+Pro" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Fira+Sans" rel="stylesheet">

<!-- Include CSS SCSS -->

   <link rel="stylesheet" type="text/css" href="/style/post.css" />
   <link rel="stylesheet" type="text/css" href="/css/monokai.css" />
   <link rel="stylesheet" type="text/css" href="/css/mystyle.css" />
   <!-- <link rel="stylesheet" type="text/css" href="/css/github.css" /> -->


<title>Bài 4: K-means Clustering</title>
<!-- <script>
var pageProperties = {
    
    category: "Clustering",
    
    url: "/2017/01/01/kmeans/",
    title: "Bài 4: K-means Clustering",
    scripts: [
        
    ],
};

</script>
<script src="/scripts/modules.js" async></script>
 -->

<link rel="icon" type="image/png" href="https://raw.githubusercontent.com/tiepvupsu/tiepvupsu.github.io/master/assets/latex/new_logo9.png" sizes="32x32">
<link rel="canonical" href="https://machinelearningcoban.com/2017/01/01/kmeans/"/>
<meta name="author" content="Tiep Vu " />


   <meta property="og:title" content="Bài 4: K-means Clustering" />
   <meta property="og:site_name" content="Tiep Vu's blog" />
   <meta property="og:url" content="https://machinelearningcoban.com/2017/01/01/kmeans/" />
   <meta property="og:description" content="" />
   
   <meta property="og:type" content="article" />
   <meta property="article:published_time" content="2017-01-01" />
   

   <meta property="article:author" content="Tiep Vu" />
   <meta property="article:section" content="Clustering" />
   
      <meta property="article:tag" content="Clustering" />
   
      <meta property="article:tag" content="Kmeans" />
   
      <meta property="article:tag" content="Unsupervised-learning" />
   


<link rel="alternate" type="application/atom+xml" title="Tiep Vu's blog - Atom feed" href="/feed.xml" />



<!-- Google Analytics -->

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-89509207-1', 'auto');
// ga('send', 'pageview');
ga('send', 'pageview', {
'page': '/2017/01/01/kmeans/',
'title': 'Bài 4: K-means Clustering'
});
</script>


<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-KTCD8BX');</script>
<!-- End Google Tag Manager -->
</head>

<body>
	<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>
    <br>
    <div class="container">
      	<div class="row">
	        <div class="col-md-2 hidden-xs hidden-sm">
	          	<a   href="/">
            <!-- <img width="80%" src="/images/logo.svg" /> -->
            <!-- <img width="100%" src="/images/logoTet.png" /> -->
            <!-- <img width="100%" src="/images/logo2.png" /> -->
            <!-- <img width="100%" style="padding-bottom: 3mm;" src="/images/logo_new.png" /> </a> -->
            <img width="100%" style="padding-bottom: 3mm;" src="/assets/latex/new_logo92.png" /> </a>
          <!-- <img width="100%" style="padding-bottom: 3mm;" src="/assets/latex/new_logo2_rau.png" /> </a> -->

            <br>
            <a href = "/buymeacoffee">
            <img width="100%" style="padding-bottom: 3mm;" src="/images/Buymeacoffee_blue.png" />

            <br>
            <a href = "/ebook">
            <img width="100%" style="padding-bottom: 3mm;" src="/images/ebook_logo.png" />


            <!-- <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#074B80', 
            'A40822MV');kofiwidget2.draw();</script>  --><!-- 
            <form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_top">
            <input type="hidden" name="cmd" value="_donations">
            <input type="hidden" name="business" value="vuhuutiep@gmail.com">
            <input type="hidden" name="lc" value="US">
            <input type="hidden" name="item_name" value="I find machinelearningcoban.com helpful. I'd like to buy Tiep Vu a coffee ^^. (Thank you so much for your support.)">
            <input type="hidden" name="no_note" value="0">
            <input type="hidden" name="currency_code" value="USD">
            <input type="hidden" name="bn" value="PP-DonationsBF:Buymeacoffee.png:NonHostedGuest">
            <input type="image" src="/images/Buymeacoffee_blue.png" border="0" style="padding-bottom: -9mm;" width = 100% name="submit" alt="PayPal - The safer, easier way to pay online!">
            </form> -->

            <!-- <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#805007', 'A40822MV');kofiwidget2.draw();</script>  -->

          </a>

          <!-- Google search -->
         <!--  <table border="0">
          <div id = "top-widget" style="width: 292px; margin-left: -13.5px; margin-top: -10px; margin-bottom: -15px;">
         <script>
           (function() {
             var cx = '012053542614118746585:ktgei4l2oek';
             var gcse = document.createElement('script');
             gcse.type = 'text/javascript';
             gcse.async = true;
             gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
             var s = document.getElementsByTagName('script')[0];
             s.parentNode.insertBefore(gcse, s);
           })();
         </script>
         <gcse:search></gcse:search>
          </div>
          </table> -->

          <!-- <nav>
          
            <div class="header">Popular</div>
            <ul>
              <li> (**): > 10k views</li>
              <li> (*) : > 5k views</li>
            </ul>
          </nav> -->
          

          
          <nav>
          
            <div class="header">Latest by category</div>
            <ul>
              
                
              
                
              
                
                  
                    <li><a style="text-align: left; color: #074B80;" href="/2017/01/04/kmeans2/">5. K-means Clustering - Applications</a></li>
                  
                    <li><a style="text-align: left; color: #074B80;" href="/2017/01/01/kmeans/">4. K-means Clustering</a></li>
                  
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
            </ul>
          </nav>
          



          <nav>
            <div class="header">Latest</div>
              
                <li><a style="text-align: left; color: #074B80"  href="/lifesofar2/">Con đường học PhD của tôi</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2018/10/03/conv2d">37. Tích chập hai chiều</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2018/09/11/forum/">Diễn đàn</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2018/07/06/deeplearning/">36. Keras</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2018/06/22/deeplearning/">35. Lược sử Deep Learning</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2018/03/22/phuonghoagiang/">Con đường học Khoa học dữ liệu của một sinh viên Kinh tế</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2018/01/14/id3/">34. Decision Trees (1): ID3</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/08/31/evaluation/">33. Đánh giá hệ thống phân lớp</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/10/20/fundaml_vectors/">FundaML 3: Các mảng ngẫu nhiên</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/10/20/fundaml_matrices/">FundaML 2: Ma trận</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/10/12/fundaml_vectors/">FundaML 1: Mảng một chiều</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/09/24/fundaml/">FundaML.com</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/08/08/nbc/">32. Naive Bayes Classifier</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/08/05/phdlife/">Viết và nhận xét các bài báo khoa học</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/07/17/mlemap/">31. Maximum Likelihood và Maximum A Posteriori</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/lifesofar/">Con đường học Toán của tôi</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/07/09/prob/">30. Ôn tập Xác Suất</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/07/02/tl/">Q2. Transfer Learning</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/06/30/lda/">29. Linear Discriminant Analysis</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/06/22/qns1/">Q1. Quick Notes 1</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/06/21/pca2/">28. Principal Component Analysis (2/2)</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/06/15/pca/">27. Principal Component Analysis (1/2)</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/06/07/svd/">26. Singular Value Decomposition</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/05/31/matrixfactorization/">25. Matrix Factorization Collaborative Filtering</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/05/24/collaborativefiltering/">24. Neighborhood-Based Collaborative Filtering</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/05/17/contentbasedrecommendersys/">23. Content-based Recommendation Systems</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/04/28/multiclasssmv/">22. Multi-class SVM</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/04/22/kernelsmv/">21. Kernel SVM</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/04/13/softmarginsmv/">20. Soft Margin SVM</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/04/09/smv/">19. Support Vector Machine</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/04/02/duality/">18. Duality</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/03/19/convexopt/">17. Convex Optimization Problems</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/03/12/convexity/">16. Convex sets và convex functions</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/03/04/overfitting/">15. Overfitting</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/02/24/mlp/">14. Multi-layer Perceptron và Backpropagation</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/02/17/softmax/">13. Softmax Regression</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/02/11/binaryclassifiers/">12. Binary Classifiers</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/general/2017/02/06/featureengineering/">11. Feature Engineering</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/02/02/howdoIcreatethisblog/"></a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/01/27/logisticregression/">10. Logistic Regression</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/01/21/perceptron/">9. Perceptron Learning Algorithm</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/01/16/gradientdescent2/">8. Gradient Descent (2/2)</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/01/12/gradientdescent/">7. Gradient Descent (1/2)</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/01/08/knn/">6. K-nearest neighbors</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/01/04/kmeans2/">5. K-means Clustering - Applications</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2017/01/01/kmeans/">4. K-means Clustering</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2016/12/28/linearregression/">3. Linear Regression</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2016/12/27/categories/">2. Phân nhóm các thuật toán Machine Learning</a></li>
              
                <li><a style="text-align: left; color: #074B80"  href="/2016/12/26/introduce/">1. Giới thiệu về Machine Learning</a></li>
              
            </ul>
          </nav>

          
          
          <!-- <img style = "transform: scaleX(1); width:100%; margin-left:00px;position: absolute;" src = "/images/mai.jpg"> -->
         
         <!--   
            <nav>
              <div class="header">Previous by date</div>
              <ul>
                <li><a style="text-align: left; font-family: 'Roboto Condensed', sans-serif; color: #074B80;" href="/2016/12/28/linearregression/">Bài 3: Linear Regression</a></li>
              </ul>
            </nav>
           
           
            <nav>
              <div class="header">Next by date</div>
              <ul>
                <li><a style="text-align: left; font-family: 'Roboto Condensed', sans-serif; color: #074B80;" href="/2017/01/04/kmeans2/">Bài 5: K-means Clustering: Simple Applications</a></li>
              </ul>
            </nav>
            -->
		       
	        </div>
	        <div class="col-md-8 col-xs-12" style = "z-index: 1">
	        	 <!-- <br> -->
 <nav class="navbar navbar-inverse" style="background-color: #074B80">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span> 
      </button>
      <a class="navbar-brand" href="/"><span style = "color: #fff">Machine Learning cơ bản</span></a>
        <!-- <form class="navbar-form navbar-left" role="search">
            <div class="form-group" align="right">
                <input type="text" class="form-control" placeholder="Search">
            </div>
            <button type="submit" class="btn btn-default">
                <span></span>
            </button>
        </form> -->
        


    </div>
    <div class="collapse navbar-collapse navbar-right" id="myNavbar">
      <ul class="nav navbar-nav">
        <li><a href="/about/" ><span style = "color: #fff"> About</span></a></li>
        <li><a href="/index/"><span style = "color: #fff">Index</span></a></li>
        <li><a href="/tags/"><span style = "color: #fff">Tags</span></a></li>
        <li><a href="/categories/"><span style = "color: #fff">Categories</span></a></li>
        <li><a href="/archive/"><span style = "color: #fff">Archive</span></a></li>
        <li><a href="/math/"><span style = "color: #fff">Math</span></a></li>
        <!-- <li><a href="https://docs.google.com/forms/d/e/1FAIpQLScq3GkxM1I2fDevR7gth-O9QqxM7grf4AFc0WT1hFORv4flaw/viewform"><span style = "color: #fff">Survey</span></a></li> -->
        <li><a href="/copyrights/"><span style = "color: #fff">Copyrights</span></a></li>
        <!-- <li><a href="/faqs/"><span style = "color: #fff">FAQs</span></a></li> -->
        <li><a href="/ebook/"><span style = "color: #fff">ebook</span></a></li>
        <li><a href="/search/"><span style = "color: #fff">Search</span></a></li>
        <!-- <li><a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/latex/book.pdf"><span style = "color: #fff">Book</span></a></li> -->
        <!-- <li><a href="https://www.facebook.com/groups/257768141347267/"><span style = "color: #fff">Forum</span></a></li> -->
        <!-- <li><a href="/subscribe/">Subscribe</a></li> -->

        <li> 
      </ul>
    </div>
  </div>
</nav>

	            <!-- <div class = "row"> -->
   <!-- <div class = "col-xs-12 hidden-md hidden-lg"> -->
      <!-- previous and next posts -->
      <div class="PageNavigation">
         
            <a class="prev" style = "color: #074B80;" href="/2016/12/28/linearregression/">&laquo; Bài 3: Linear Regression</a>
         <!-- <hr> -->
         
         
            <a class="next" style = "float: right; color: #074B80;" href="/2017/01/04/kmeans2/">Bài 5: K-means Clustering: Simple Applications &raquo;</a>
         <hr>
         
      </div>
  <!-- </div> -->
<!-- </div> -->
<h1 itemprop="name" class="post-title">Bài 4: K-means Clustering</h1>


<ul class = "tags">
   
      <a href="/tags#Clustering" class="tag">Clustering</a>
   
      <a href="/tags#Kmeans" class="tag">Kmeans</a>
   
      <a href="/tags#Unsupervised-learning" class="tag">Unsupervised-learning</a>
   
</ul>



<span class = "post-date" style="color: gray; font-style: italic;">Jan 1, 2017
            </span>
<!-- Main content -->
<br>
<br>



<div itemprop="articleBody">
   <!-- 
<div class="imgcap">
<div >
<a href = "/2017/01/01/kmeans/">
    <img src ="/assets/kmeans/kmeans11.gif"  width = "800"></a>
</div>
<div class="thecap"> K-means Clustering <br></div>
</div> -->

<p><strong>Trong trang này:</strong>
<!-- MarkdownTOC --></p>

<ul>
  <li><a href="#-gioi-thieu">1. Giới thiệu</a></li>
  <li><a href="#-phan-tich-toan-hoc">2. Phân tích toán học</a>
    <ul>
      <li><a href="#mot-so-ky-hieu-toan-hoc">Một số ký hiệu toán học</a></li>
      <li><a href="#ham-mat-mat-va-bai-toan-toi-uu">Hàm mất mát và bài toán tối ưu</a></li>
      <li><a href="#thuat-toan-toi-uu-ham-mat-mat">Thuật toán tối ưu hàm mất mát</a>
        <ul>
          <li><a href="#co-dinh-\\\mathbfm-\\-tim-\\\mathbfy\\">Cố định \(\mathbf{M} \), tìm \(\mathbf{Y}\)</a></li>
          <li><a href="#co-dinh-\\\mathbfy-\\-tim-\\\mathbfm\\">Cố định \(\mathbf{Y} \), tìm \(\mathbf{M}\)</a></li>
        </ul>
      </li>
      <li><a href="#tom-tat-thuat-toan">Tóm tắt thuật toán</a></li>
    </ul>
  </li>
  <li><a href="#-vi-du-tren-python">3. Ví dụ trên Python</a>
    <ul>
      <li><a href="#gioi-thieu-bai-toan">Giới thiệu bài toán</a></li>
      <li><a href="#hien-thi-du-lieu-tren-do-thi">Hiển thị dữ liệu trên đồ thị</a></li>
      <li><a href="#cac-ham-so-can-thiet-cho-k-means-clustering">Các hàm số cần thiết cho K-means clustering</a></li>
      <li><a href="#ket-qua-tim-duoc-bang-thu-vien-scikit-learn">Kết quả tìm được bằng thư viện scikit-learn</a></li>
    </ul>
  </li>
  <li><a href="#-thao-luan">4. Thảo luận</a>
    <ul>
      <li><a href="#han-che">Hạn chế</a>
        <ul>
          <li><a href="#chung-ta-can-biet-so-luong-cluster-can-clustering">Chúng ta cần biết số lượng cluster cần clustering</a></li>
          <li><a href="#nghiem-cuoi-cung-phu-thuoc-vao-cac-centers-duoc-khoi-tao-ban-dau">Nghiệm cuối cùng phụ thuộc vào các centers được khởi tạo ban đầu</a></li>
          <li><a href="#cac-cluster-can-co-so-luong-diem-gan-bang-nhau">Các cluster cần có só lượng điểm gần bằng nhau</a></li>
          <li><a href="#cac-cluster-can-co-dang-hinh-tron">Các cluster cần có dạng hình tròn</a></li>
          <li><a href="#khi-mot-cluster-nam-phia-trong--cluster-khac">Khi một cluster nằm phía trong 1 cluster khác</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#-tai-lieu-tham-khao">5. Tài liệu tham khảo</a></li>
</ul>

<!-- /MarkdownTOC -->

<p><a name="-gioi-thieu"></a></p>

<h2 id="1-giới-thiệu">1. Giới thiệu</h2>

<p><a href="/2016/12/28/linearregression/">Trong bài trước</a>, chúng ta đã làm quen với thuật toán Linear Regression - là thuật toán đơn giản nhất trong <a href="/2016/12/27/categories/#supervised-learning-hoc-co-giam-sat">Supervised learning</a>. Bài này tôi sẽ giới thiệu một trong những thuật toán cơ bản nhất trong <a href="/2016/12/27/categories/#unsupervised-learning-hoc-khong-giam-sat">Unsupervised learning</a> - thuật toán K-means clustering (phân cụm K-means).</p>

<p>Trong thuật toán K-means clustering, chúng ta không biết nhãn (label) của từng điểm dữ liệu. Mục đích là làm thể nào để phân dữ liệu thành các cụm (cluster) khác nhau sao cho <em>dữ liệu trong cùng một cụm có tính chất giống nhau</em>.</p>

<p><strong>Ví dụ:</strong> Một công ty muốn tạo ra những chính sách ưu đãi cho những nhóm khách hàng khác nhau dựa trên sự tương tác giữa mỗi khách hàng với công ty đó (số năm là khách hàng; số tiền khách hàng đã chi trả cho công ty; độ tuổi; giới tính; thành phố; nghề nghiệp; …). Giả sử công ty đó có rất nhiều dữ liệu của rất nhiều khách hàng nhưng chưa có cách nào chia toàn bộ khách hàng đó thành một số nhóm/cụm khác nhau. Nếu một người biết Machine Learning được đặt câu hỏi này, phương pháp đầu tiên anh (chị) ta nghĩ đến sẽ là K-means Clustering. Vì nó là một trong những thuật toán đầu tiên mà anh ấy tìm được trong các cuốn sách, khóa học về Machine Learning. Và tôi cũng chắc rằng anh ấy đã đọc blog <a href="https://tiepvupsu.github.io">Machine Learning cơ bản</a>. Sau khi đã phân ra được từng nhóm, nhân viên công ty đó có thể lựa chọn ra một vài khách hàng trong mỗi nhóm để quyết định xem mỗi nhóm tương ứng với nhóm khách hàng nào. Phần việc cuối cùng này cần sự can thiệp của con người, nhưng lượng công việc đã được rút gọn đi rất nhiều.</p>

<p>Ý tưởng đơn giản nhất về cluster (cụm) là tập hợp các điểm <em>ở gần nhau trong một không gian nào đó</em> (không gian này có thể có rất nhiều chiều trong trường hợp thông tin về một điểm dữ liệu là rất lớn). Hình bên dưới là một ví dụ về 3 cụm dữ liệu (từ giờ tôi sẽ viết gọn là <em>cluster</em>).</p>

<div class="imgcap">
<img src="/assets/kmeans/figure_2.png" width="500" align="center" />
<div class="thecap"> Bài toán với 3 clusters. <br /></div>
</div>

<p>Giả sử mỗi cluster có một điểm đại diện (<em>center</em>) màu vàng. Và những điểm xung quanh mỗi center thuộc vào cùng nhóm với center đó. Một cách đơn giản nhất, xét một điểm bất kỳ, ta xét xem điểm đó gần với center nào nhất thì nó thuộc về cùng nhóm với center đó. Tới đây, chúng ta có một bài toán thú vị: <em>Trên một vùng biển hình vuông lớn có ba đảo hình vuông, tam giác, và tròn màu vàng như hình trên. Một điểm trên biển được gọi là thuộc lãnh hải của một đảo nếu nó nằm gần đảo này hơn so với hai đảo kia . Hãy xác định ranh giới lãnh hải của các đảo.</em></p>

<p>Hình dưới đây là một hình minh họa cho việc phân chia lãnh hải nếu có 5 đảo khác nhau được biểu diễn bằng các hình tròn màu đen:</p>
<div class="imgcap">
<img src="/assets/kmeans/figure_1.png" width="500" align="center" />
<div class="thecap"> Phân vùng lãnh hải của mỗi đảo. Các vùng khác nhau có màu sắc khác nhau. <br /></div>
</div>

<p>Chúng ta thấy rằng đường phân định giữa các lãnh hải là các đường thẳng (chính xác hơn thì chúng là các đường trung trực của các cặp điểm gần nhau). Vì vậy, lãnh hải của một đảo sẽ là một hình đa giác.</p>

<p>Cách phân chia này trong toán học được gọi là <a href="https://en.wikipedia.org/wiki/Voronoi_diagram">Voronoi Diagram</a>.</p>

<p>Trong không gian ba chiều, lấy ví dụ là các hành tinh, thì (tạm gọi là) <em>lãnh không</em> của mỗi hành tinh sẽ là một đa diện. Trong không gian nhiều chiều hơn, chúng ta sẽ có những thứ (mà tôi gọi là) <em>siêu đa diện</em> (hyperpolygon).</p>

<p>Quay lại với bài toán phân nhóm và cụ thể là thuật toán K-means clustering, chúng ta cần một chút phân tích toán học trước khi đi tới phần <a href="#tom-tat-thuat-toan">tóm tắt thuật toán</a> ở phần dưới. Nếu bạn không muốn đọc quá nhiều về toán, bạn có thể bỏ qua phần này. (<em>Tốt nhất là đừng bỏ qua, bạn sẽ tiếc đấy</em>).
<!-- ========================== New Heading ==================== -->
<a name="-phan-tich-toan-hoc"></a></p>

<h2 id="2-phân-tích-toán-học">2. Phân tích toán học</h2>

<p>Mục đích cuối cùng của thuật toán phân nhóm này là: từ dữ liệu đầu vào và số lượng nhóm chúng ta muốn tìm, hãy chỉ ra center của mỗi nhóm và phân các điểm dữ liệu vào các nhóm tương ứng. Giả sử thêm rằng mỗi điểm dữ liệu chỉ thuộc vào đúng một nhóm.</p>

<p><a name="mot-so-ky-hieu-toan-hoc"></a></p>

<h3 id="một-số-ký-hiệu-toán-học">Một số ký hiệu toán học</h3>
<p>Giả sử có \(N\) điểm dữ liệu là \( \mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N] \in \mathbb{R}^{d \times N}\) và \(K &lt; N\) là số cluster chúng ta muốn phân chia. Chúng ta cần tìm các center \( \mathbf{m}_1, \mathbf{m}_2, \dots, \mathbf{m}_K \in \mathbb{R}^{d \times 1} \) và label của mỗi điểm dữ liệu.</p>

<p><strong>Lưu ý về ký hiệu toán học:</strong> <em>trong các bài viết của tôi, các số vô hướng được biểu diễn bởi các chữ cái viết ở dạng không in đậm, có thể viết hoa, ví dụ \(x_1, N, y, k\). Các vector được biểu diễn bằng các chữ cái thường in đậm, ví dụ \(\mathbf{m}, \mathbf{x}_1 \). Các ma trận được biểu diễn bởi các chữ viết hoa in đậm, ví dụ \(\mathbf{X, M, Y} \). Lưu ý này đã được nêu ở bài <a href="/2016/12/28/linearregression/">Linear Regression</a>. Tôi xin được không nhắc lại trong các bài tiếp theo.</em></p>

<p><a name="one-hot"></a>
Với mỗi điểm dữ liệu \( \mathbf{x}_i \) đặt \(\mathbf{y}_i = [y_{i1}, y_{i2}, \dots, y_{iK}]\) là label vector của nó, trong đó nếu \( \mathbf{x}_i \) được phân vào cluster \(k\) thì  \(y_{ik} = 1\) và \(y_{ij} = 0, \forall j \neq k \). Điều này có nghĩa là có đúng một phần tử của vector \(\mathbf{y}_i\) là bằng 1 (tương ứng với cluster của \(\mathbf{x}_i \)), các phần tử còn lại bằng 0. Ví dụ: nếu một điểm dữ liệu có label vector là \([1,0,0,\dots,0]\) thì nó thuộc vào cluster 1, là \([0,1,0,\dots,0]\) thì nó thuộc vào cluster 2, \(\dots\). Cách mã hóa label của dữ liệu như thế này được gọi là biểu diễn <a href="https://en.wikipedia.org/wiki/One-hot"><em>one-hot</em></a>. Chúng ta sẽ thấy cách biểu diễn one-hot này rất phổ biến trong Machine Learning ở các bài tiếp theo.</p>

<p>Ràng buộc của \(\mathbf{y}_i \) có thể viết dưới dạng toán học như sau:
\[
 y_{ik} \in \{0, 1\},~~~ \sum_{k = 1}^K y_{ik} = 1 ~~~ (1)
\]
<!-- \\[ \mathbf{y}_i = [y_{i}, y_2] \\] -->
<!-- [y_{i1}]\\) -->
<!-- đặt \\(\mathbf{y}_i = [y_{i1}, y_{i2}, \dots, y_{iK} \\) là vector biểu diễn label của một điểm, trong đó nếu nếu điểm dữ liệu --></p>

<p><!-- \\(\mathbf{x}_i\\) được phân vào cluster \\(k\\) thì  \\(y_{ik} = 1\\)  -->
<!-- ========================== New Heading ==================== -->
<a name="ham-mat-mat-va-bai-toan-toi-uu"></a></p>

<h3 id="hàm-mất-mát-và-bài-toán-tối-ưu">Hàm mất mát và bài toán tối ưu</h3>

<p>Nếu ta coi center \(\mathbf{m}_k \)  là center (hoặc representative) của mỗi cluster và <em>ước lượng</em> tất cả các điểm được phân vào cluster này bởi \(\mathbf{m}_k \), thì một điểm dữ liệu \(\mathbf{x}_i \) được phân vào cluster \(k\) sẽ bị sai số là \( (\mathbf{x}_i - \mathbf{m}_k) \). Chúng ta mong muốn sai số này có trị tuyệt đối nhỏ nhất nên (<a href="/2016/12/28/linearregression/#sai-so-du-doan">giống như trong bài Linear Regression</a>) ta sẽ tìm cách để đại lượng sau đây đạt giá trị nhỏ nhất: 
\[
\|\mathbf{x}_i - \mathbf{m}_k\|_2^2
\]</p>

<p>Hơn nữa, vì \(\mathbf{x}_i \) được phân vào cluster \(k\) nên \(y_{ik} = 1, y_{ij} = 0, ~\forall j \neq k \). Khi đó, biểu thức bên trên sẽ được viết lại là:
\[
y_{ik}\|\mathbf{x}_i - \mathbf{m}_k\|_2^2 =  \sum_{j=1}^K y_{ij}\|\mathbf{x}_i - \mathbf{m}_j\|_2^2
\]</p>

<p>(<em>Hy vọng chỗ này không quá khó hiểu</em>)</p>

<!-- Với mỗi điểm dữ liệu \\(\mathbf{x}\_i \\), vì chưa biết nó thuộc cluster nào, (_ở đây chúng ta tạm thời giả sử rằng ta đã biết các center_), thế thì hàm mất mát của cách phân cụm cho điểm \\(\mathbf{x}\_i \\) sẽ là: -->

<!-- \\[
\sum\_{k=1}^K y\_{ik} \\\|\mathbf{x}\_i - \mathbf{m}\_k \\|_2^2
\\]
 -->

<p>Sai số cho toàn bộ dữ liệu sẽ là: 
\[
\mathcal{L}(\mathbf{Y}, \mathbf{M}) = \sum_{i=1}^N\sum_{j=1}^K y_{ij} \|\mathbf{x}_i - \mathbf{m}_j\|_2^2
\]</p>

<p>Trong đó \( \mathbf{Y} = [\mathbf{y}_1; \mathbf{y}_2; \dots; \mathbf{y}_N]\), \( \mathbf{M} = [\mathbf{m}_1, \mathbf{m}_2, \dots \mathbf{m}_K] \) lần lượt là các ma trận được tạo bởi label vector của mỗi điểm dữ liệu và center của mỗi cluster. Hàm số mất mát trong bài toán K-means clustering của chúng ta là hàm \(\mathcal{L}(\mathbf{Y}, \mathbf{M})\) với ràng buộc như được nêu trong phương trình \((1)\).</p>

<p>Tóm lại, chúng ta cần tối ưu bài toán sau: 
\[
\mathbf{Y}, \mathbf{M} = \arg\min_{\mathbf{Y}, \mathbf{M}} \sum_{i=1}^N\sum_{j=1}^K y_{ij} \|\mathbf{x}_i - \mathbf{m}_j\|_2^2~~~~~(2)
\]</p>

<p>\[
\text{subject to:} ~~ y_{ij} \in \{0, 1\}~~ \forall i, j;~~~ \sum_{j = 1}^K y_{ij} = 1~~\forall i
\]</p>

<p>(<em>subject to</em> nghĩa là <em>thỏa mãn điều kiện</em>).</p>

<p><strong>Nhắc lại khái niệm \(\arg\min\)</strong>: Chúng ta biết ký hiệu \(\min\) là <em>giá trị nhỏ nhất của hàm số</em>, \(\arg\min\) chính là <em>giá trị của biến số để hàm số đó đạt giá trị nhỏ nhất đó</em>. Nếu \(f(x) = x^2 -2x + 1 = (x-1)^2 \) thì giá trị nhỏ nhất của hàm số này bằng 0, đạt được khi \(x = 1\). Trong ví dụ này \(\min_{x} f(x) = 0\) và \(\arg\min_{x} f(x) = 1\). Thêm ví dụ khác, nếu \(x_1 = 0, x_2 = 10, x_3 = 5\) thì ta nói \(\arg\min_{i} x_i = 1\) vì \(1\) là chỉ số để \(x_i\) đạt giá trị nhỏ nhất (bằng \(0\)). Biến số viết bên dưới \(\min\) là biến số cúng ta cần tối ưu. Trong các bài toán tối ưu, ta thường quan tâm tới \(\arg\min\) hơn là \(\min\).
<!-- _K-means còn là một trong những phương pháp thực hiện ước lượng vector [_vector quantization_](https://en.wikipedia.org/wiki/Vector_quantization)_ -->
<!-- ========================== New Heading ==================== -->
<a name="thuat-toan-toi-uu-ham-mat-mat"></a></p>

<h3 id="thuật-toán-tối-ưu-hàm-mất-mát">Thuật toán tối ưu hàm mất mát</h3>
<p>Bài toán \((2)\) là một bài toán khó tìm <em>điểm tối ưu</em> vì nó có thêm các điều kiện ràng buộc. <em>Bài toán này thuộc loại mix-integer programming (điều kiện biến là số nguyên) - là loại rất khó tìm nghiệm tối ưu toàn cục (global optimal point, tức nghiệm làm cho hàm mất mát đạt giá trị nhỏ nhất có thể).</em> Tuy nhiên, trong một số trường hợp chúng ta vẫn có thể tìm được phương pháp để tìm được nghiệm gần đúng hoặc điểm cực tiểu. (<em>Nếu chúng ta vẫn nhớ chương trình toán ôn thi đại học thì điểm cực tiểu chưa chắc đã phải là điểm làm cho hàm số đạt giá trị nhỏ nhất</em>).</p>

<p>Một cách đơn giản để giải bài toán \((2)\) là xen kẽ giải \(\mathbf{Y}\) và \( \mathbf{M}\) khi biến còn lại được cố định. Đây là một thuật toán lặp, cũng là kỹ thuật phổ biến khi giải bài toán tối ưu. Chúng ta sẽ lần lượt giải quyết hai bài toán sau đây:</p>

<!-- ========================== New Heading ==================== -->
<p><a name="co-dinh-\\\mathbfm-\\-tim-\\\mathbfy\\"></a></p>

<h4 id="cố-định-mathbfm--tìm-mathbfy">Cố định \(\mathbf{M} \), tìm \(\mathbf{Y}\)</h4>
<p><strong>Giả sử đã tìm được các centers, hãy tìm các label vector để hàm mất mát đạt giá trị nhỏ nhất.</strong> Điều này tương đương với việc tìm cluster cho mỗi điểm dữ liệu.</p>

<p>Khi các centers là cố định, bài toán tìm label vector cho toàn bộ dữ liệu có thể được chia nhỏ thành bài toán tìm label vector cho từng điểm dữ liệu \(\mathbf{x}_i\) như sau:</p>

<p>\[
\mathbf{y}_i = \arg\min_{\mathbf{y}_i} \sum_{j=1}^K y_{ij}\|\mathbf{x}_i - \mathbf{m}_j\|_2^2 ~~~ (3)
\]
\[
\text{subject to:} ~~ y_{ij} \in \{0, 1\}~~ \forall j;~~~ \sum_{j = 1}^K y_{ij} = 1
\]</p>

<p>Vì chỉ có một phần tử của label vector \(\mathbf{y}_i\) bằng \(1\) nên bài toán \((3)\) có thể tiếp tục được viết dưới dạng đơn giản hơn: 
\[
j = \arg\min_{j} \|\mathbf{x}_i - \mathbf{m}_j\|_2^2
\]</p>

<p>Vì \(\|\mathbf{x}_i - \mathbf{m}_j\|_2^2\) chính là bình phương khoảng cách tính từ điểm \(\mathbf{x}_i \) tới center \(\mathbf{m}_j \), ta có thể kết luận rằng <strong>mỗi điểm \(\mathbf{x}_i \) thuộc vào cluster có center gần nó nhất</strong>! Từ đó ta có thể dễ dàng suy ra label vector của từng điểm dữ liệu.</p>

<!-- ========================== New Heading ==================== -->
<p><a name="co-dinh-\\\mathbfy-\\-tim-\\\mathbfm\\"></a></p>

<h4 id="cố-định-mathbfy--tìm-mathbfm">Cố định \(\mathbf{Y} \), tìm \(\mathbf{M}\)</h4>
<p><strong>Giả sử đã tìm được cluster cho từng điểm, hãy tìm center mới cho mỗi cluster để hàm mất mát đạt giá trị nhỏ nhất.</strong></p>

<p>Một khi chúng ta đã xác định được label vector cho từng điểm dữ liệu, bài toán tìm center cho mỗi cluster được rút gọn thành:</p>

<p>\[
\mathbf{m}_j = \arg\min_{\mathbf{m}_j} \sum_{i = 1}^{N} y_{ij}\|\mathbf{x}_i - \mathbf{m}_j \|_2^2.
\]
 Tới đây, ta có thể tìm nghiệm bằng phương pháp giải đạo hàm bằng 0, vì hàm cần tối ưu là một hàm liên tục và có đạo hàm xác định tại mọi điểm. <em>Và quan trọng hơn, hàm này là hàm convex (lồi) theo \(\mathbf{m}_j \) nên chúng ta sẽ tìm được giá trị nhỏ nhất và điểm tối ưu tương ứng. Sau này nếu có dịp, tôi sẽ nói thêm về tối ưu lồi (convex optimization) - một mảng cực kỳ quan trọng trong toán tối ưu</em>.</p>

<p>Đặt \(l(\mathbf{m}_j)\) là hàm bên trong dấu \(\arg\min\), ta có đạo hàm:
\[
\frac{\partial l(\mathbf{m}_j)}{\partial \mathbf{m}_j} = 2\sum_{i=1}^N y_{ij}(\mathbf{m}_j - \mathbf{x}_i) 
\]</p>

<p>Giải phương trình đạo hàm bằng 0 ta có: 
\[
\mathbf{m}_j \sum_{i=1}^N y_{ij} = \sum_{i=1}^N y_{ij} \mathbf{x}_i 
\]
\[
\Rightarrow \mathbf{m}_j = \frac{ \sum_{i=1}^N y_{ij} \mathbf{x}_i}{\sum_{i=1}^N y_{ij}}
\]</p>

<p>Nếu để ý một chút, chúng ta sẽ thấy rằng mẫu số chính là phép đếm <em>số lượng các điểm dữ liệu</em> trong cluster \(j\) (<em>Bạn có nhận ra không?</em>). Còn tử số chính là <em>tổng các điểm dữ liệu</em> trong cluster \(j\). (<em>Nếu bạn đọc vẫn nhớ điều kiện ràng buộc của các</em> \(y_{ij} \) <em>thì sẽ có thể nhanh chóng nhìn ra điều này</em>).</p>

<p>Hay nói một cách đơn giản hơn nhiều: \(\mathbf{m}_j\) <strong>là trung bình cộng của các điểm trong cluster</strong> \(j\).</p>

<p>Tên gọi <em>K-means clustering</em> cũng xuất phát từ đây.
<!-- ========================== New Heading ==================== -->
<a name="tom-tat-thuat-toan"></a></p>

<p><a name="tom-tat-thuat-toan"></a></p>
<h3 id="tóm-tắt-thuật-toán">Tóm tắt thuật toán</h3>
<p>Tới đây tôi xin được tóm tắt lại thuật toán (<em>đặc biệt quan trọng với các bạn bỏ qua phần toán học bên trên</em>) như sau:</p>

<p><strong>Đầu vào:</strong> Dữ liệu \(\mathbf{X}\) và số lượng cluster cần tìm \(K\).</p>

<p><strong>Đầu ra:</strong> Các center \(\mathbf{M}\) và label vector cho từng điểm dữ liệu \(\mathbf{Y}\).</p>

<ol>
  <li>Chọn \(K\) điểm bất kỳ làm các center ban đầu.</li>
  <li>Phân mỗi điểm dữ liệu vào cluster có center gần nó nhất.</li>
  <li>Nếu việc gán dữ liệu vào từng cluster ở bước 2 không thay đổi so với vòng lặp trước nó thì ta dừng thuật toán.</li>
  <li>Cập nhật center cho từng cluster bằng cách lấy trung bình cộng của tất các các điểm dữ liệu đã được gán vào cluster đó sau bước 2.</li>
  <li>Quay lại bước 2.</li>
</ol>

<p>Chúng ta có thể đảm bảo rằng thuật toán sẽ dừng lại sau một số hữu hạn vòng lặp. Thật vậy, vì hàm mất mát là một số dương và sau mỗi bước 2 hoặc 3, giá trị của hàm mất mát bị giảm đi. Theo kiến thức về dãy số trong chương trình cấp 3: <em>nếu một dãy số giảm và bị chặn dưới thì nó hội tụ!</em> Hơn nữa, số lượng cách phân nhóm cho toàn bộ dữ liệu là hữu hạn nên đến một lúc nào đó, hàm mất mát sẽ không thể thay đổi, và chúng ta có thể dừng thuật toán tại đây.</p>

<p>Chúng ta sẽ có một vài <a href="#-thao-luan">thảo luận</a> về thuật toán này, về những hạn chế và một số phương pháp khắc phục. Nhưng trước hết, hãy xem nó thể hiện như thế nào trong một ví dụ cụ thể dưới đây.</p>

<!-- ========================== New Heading ==================== -->
<p><a name="-vi-du-tren-python"></a></p>

<h2 id="3-ví-dụ-trên-python">3. Ví dụ trên Python</h2>

<!-- ========================== New Heading ==================== -->
<p><a name="gioi-thieu-bai-toan"></a></p>

<h3 id="giới-thiệu-bài-toán">Giới thiệu bài toán</h3>

<p>Để kiểm tra mức độ hiểu quả của một thuật toán, chúng ta sẽ làm một ví dụ đơn giản (thường được gọi là <em>toy example</em>). Trước hết, chúng ta chọn center cho từng cluster và tạo dữ liệu cho từng cluster bằng cách lấy mẫu theo phân phối chuẩn có kỳ vọng là center của cluster đó và ma trận hiệp phương sai (covariance matrix) là ma trận đơn vị.</p>

<p>Trước tiên, chúng ta cần khai báo các thư viện cần dùng. Chúng ta cần <code class="language-plaintext highlighter-rouge">numpy</code> và <code class="language-plaintext highlighter-rouge">matplotlib</code> như trong bài <a href="/2016/12/28/linearregression/">Linear Regression</a> cho việc tính toán ma trận và hiển thị dữ liệu. Chúng ta cũng cần thêm thư viện <code class="language-plaintext highlighter-rouge">scipy.spatial.distance</code> để tính khoảng cách giữa các cặp điểm trong hai tập hợp một cách hiệu quả.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span> 
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>
</code></pre></div></div>

<p>Tiếp theo, ta tạo dữ liệu bằng cách lấy các điểm theo phân phối chuẩn có kỳ vọng tại các điểm có tọa độ <code class="language-plaintext highlighter-rouge">(2, 2), (8, 3)</code> và <code class="language-plaintext highlighter-rouge">(3, 6)</code>, ma trận hiệp phương sai giống nhau và là ma trận đơn vị. Mỗi cluster có 500 điểm. (<em>Chú ý rằng mỗi điểm dữ liệu là một hàng của ma trận dữ liệu.</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">means</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">X0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">original_label</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">N</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">N</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">N</span><span class="p">).</span><span class="n">T</span>
</code></pre></div></div>

<!-- ========================== New Heading ==================== -->
<p><a name="hien-thi-du-lieu-tren-do-thi"></a></p>

<h3 id="hiển-thị-dữ-liệu-trên-đồ-thị">Hiển thị dữ liệu trên đồ thị</h3>

<p>Chúng ta cần một hàm <code class="language-plaintext highlighter-rouge">kmeans_display</code> để hiển thị dữ liệu. Sau đó hiển thị dữ liệu theo nhãn ban đầu.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">kmeans_display</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">amax</span><span class="p">(</span><span class="n">label</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">X0</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">X1</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">X2</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="p">:]</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'b^'</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="p">.</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'go'</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="p">.</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'rs'</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="p">.</span><span class="mi">8</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">kmeans_display</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">original_label</span><span class="p">)</span>
</code></pre></div></div>

<!-- ![png](output_5_0.png) -->
<div class="imgcap">
<img src="/assets/kmeans/output_5_0.png" align="center" />
</div>
<p>Trong đồ thị trên, mỗi cluster tương ứng với một màu. Có thể nhận thấy rằng có một vài điểm màu đỏ bị lẫn sang phần cluster màu xanh.
<!-- ========================== New Heading ==================== -->
<a name="cac-ham-so-can-thiet-cho-k-means-clustering"></a></p>

<p><a name="cac-ham-so-can-thiet-cho-k-means-clustering"></a></p>
<h3 id="các-hàm-số-cần-thiết-cho-k-means-clustering">Các hàm số cần thiết cho K-means clustering</h3>
<p>Viết các hàm:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">kmeans_init_centers</code> để khởi tạo các centers ban đầu.</li>
  <li><code class="language-plaintext highlighter-rouge">kmeans_asign_labels</code> để gán nhán mới cho các điểm khi biết các centers.</li>
  <li><code class="language-plaintext highlighter-rouge">kmeans_update_centers</code> để cập nhật các centers mới dữa trên dữ liệu vừa được gán nhãn.</li>
  <li><code class="language-plaintext highlighter-rouge">has_converged</code> để kiểm tra điều kiện dừng của thuật toán.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">kmeans_init_centers</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="c1"># randomly pick k rows of X as initial centers
</span>    <span class="k">return</span> <span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">k</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">kmeans_assign_labels</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">centers</span><span class="p">):</span>
    <span class="c1"># calculate pairwise distances btw data and centers
</span>    <span class="n">D</span> <span class="o">=</span> <span class="n">cdist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">centers</span><span class="p">)</span>
    <span class="c1"># return index of the closest center
</span>    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">kmeans_update_centers</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="c1"># collect all points assigned to the k-th cluster 
</span>        <span class="n">Xk</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">labels</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="p">:]</span>
        <span class="c1"># take average
</span>        <span class="n">centers</span><span class="p">[</span><span class="n">k</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Xk</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">centers</span>

<span class="k">def</span> <span class="nf">has_converged</span><span class="p">(</span><span class="n">centers</span><span class="p">,</span> <span class="n">new_centers</span><span class="p">):</span>
    <span class="c1"># return True if two sets of centers are the same
</span>    <span class="k">return</span> <span class="p">(</span><span class="nb">set</span><span class="p">([</span><span class="nb">tuple</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">centers</span><span class="p">])</span> <span class="o">==</span> 
        <span class="nb">set</span><span class="p">([</span><span class="nb">tuple</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">new_centers</span><span class="p">]))</span>
</code></pre></div></div>

<p>Phần chính của K-means clustering:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">kmeans</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="p">[</span><span class="n">kmeans_init_centers</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">)]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span> 
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">kmeans_assign_labels</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">centers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">new_centers</span> <span class="o">=</span> <span class="n">kmeans_update_centers</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">K</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">has_converged</span><span class="p">(</span><span class="n">centers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">new_centers</span><span class="p">):</span>
            <span class="k">break</span>
        <span class="n">centers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_centers</span><span class="p">)</span>
        <span class="n">it</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">centers</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">it</span><span class="p">)</span>
</code></pre></div></div>

<p>Áp dụng thuật toán vừa viết vào dữ liệu ban đầu, hiển thị kết quả cuối cùng.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">centers</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">it</span><span class="p">)</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Centers found by our algorithm:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">centers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">kmeans_display</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Centers found by our algorithm:
[[ 1.97563391  2.01568065]
 [ 8.03643517  3.02468432]
 [ 2.99084705  6.04196062]]
</code></pre></div></div>

<div class="imgcap">
<img src="/assets/kmeans/output_11_1.png" align="center" />
</div>

<!-- ![png](output_11_1.png) -->

<p>Từ kết quả này chúng ta thấy rằng thuật toán K-means clustering làm việc khá thành công, các centers tìm được khá gần với kỳ vọng ban đầu. Các điểm thuộc cùng một cluster hầu như được phân vào cùng một cluster (trừ một số điểm màu đỏ ban đầu đã bị phân nhầm vào cluster màu xanh da trời, nhưng tỉ lệ là nhỏ và có thể chấp nhận được).</p>

<p>Dưới đây là hình ảnh động minh họa thuật toán qua từng vòng lặp, chúng ta thấy rằng thuật toán trên hội tụ rất nhanh, chỉ cần 6 vòng lặp để có được kết quả cuối cùng:</p>
<div class="imgcap">
<img src="/assets/kmeans/kmeans11.gif" align="center" />
</div>

<p>Các bạn có thể xem thêm các trang web minh họa thuật toán K-means cluster tại:</p>
<ol>
  <li><a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">Visualizing K-Means Clustering</a></li>
  <li><a href="http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html">Visualizing K-Means Clustering - Standford</a></li>
</ol>

<!-- ========================== New Heading ==================== -->
<p><a name="ket-qua-tim-duoc-bang-thu-vien-scikit-learn"></a></p>

<p><a name="ket-qua-tim-duoc-bang-thu-vien-scikit-learn"></a></p>
<h3 id="kết-quả-tìm-được-bằng-thư-viện-scikit-learn">Kết quả tìm được bằng thư viện scikit-learn</h3>

<p>Để kiểm tra thêm, chúng ta hãy so sánh kết quả trên với kết quả thu được bằng cách sử dụng thư viện <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"><code class="language-plaintext highlighter-rouge">scikit-learn</code></a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Centers found by scikit-learn:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">)</span>
<span class="n">pred_label</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">kmeans_display</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pred_label</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Centers found by scikit-learn:
[[ 8.0410628   3.02094748]
 [ 2.99357611  6.03605255]
 [ 1.97634981  2.01123694]]
</code></pre></div></div>

<!-- ![png](output_14_1.png) -->
<div class="imgcap">
<img src="/assets/kmeans/output_14_1.png" align="center" />
</div>

<p>Thật may mắn (<em>cho tôi</em>), hai thuật toán cho cùng một đáp số! Với cách thứ nhất, tôi mong muốn các bạn hiểu rõ được thuật toán K-means clustering làm việc như thế nào. Với cách thứ hai, tôi hy vọng các bạn biết áp dụng thư viện sẵn có như thế nào.</p>

<!-- ========================== New Heading ==================== -->
<p><a name="-thao-luan"></a></p>

<h2 id="4-thảo-luận">4. Thảo luận</h2>
<!-- ========================== New Heading ==================== -->
<p><a name="han-che"></a></p>

<h3 id="hạn-chế">Hạn chế</h3>
<p>Có một vài hạn chế của thuật toán K-means clustering:</p>

<!-- ========================== New Heading ==================== -->
<p><a name="chung-ta-can-biet-so-luong-cluster-can-clustering"></a></p>

<h4 id="chúng-ta-cần-biết-số-lượng-cluster-cần-clustering">Chúng ta cần biết số lượng cluster cần clustering</h4>
<p>Để ý thấy rằng trong <a href="#tom-tat-thuat-toan">thuật toán nêu trên</a>, chúng ta cần biết đại lượng \(K\) là số lượng clusters. Trong thực tế, nhiều trường hợp chúng ta không xác định được giá trị này. Có một số phương pháp giúp xác định số lượng clusters, tôi sẽ dành thời gian nói về chúng sau nếu có dịp. Bạn đọc có thể tham khảo <a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set">Elbow method - Determining the number of clusters in a data set</a>.</p>

<!-- ========================== New Heading ==================== -->
<p><a name="nghiem-cuoi-cung-phu-thuoc-vao-cac-centers-duoc-khoi-tao-ban-dau"></a></p>

<h4 id="nghiệm-cuối-cùng-phụ-thuộc-vào-các-centers-được-khởi-tạo-ban-đầu">Nghiệm cuối cùng phụ thuộc vào các centers được khởi tạo ban đầu</h4>
<p>Tùy vào các center ban đầu mà thuật toán có thể có tốc độ hội tụ rất chậm, ví dụ:
<!-- ![png](output_14_1.png) --></p>
<div class="imgcap">
<img src="/assets/kmeans/kmeans_slowconverge.gif" align="center" />
</div>
<p>hoặc thậm chí cho chúng ta nghiệm không chính xác (chỉ là local minimum - điểm cực tiểu - mà không phải giá trị nhỏ nhất):</p>

<div class="imgcap">
<img src="/assets/kmeans/kmeans_badresult.gif" align="center" />
</div>

<p>Có một vài cách khắc phục đó là:</p>

<ul>
  <li>
    <p>Chạy K-means clustering nhiều lần với các center ban đầu khác nhau rồi chọn cách có hàm mất mát cuối cùng đạt giá trị nhỏ nhất.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/K-means%2B%2B#Improved_initialization_algorithm">K-means++ -Improve initialization algorithm - wiki</a>.</p>
  </li>
  <li>
    <p>Bạn nào muốn tìm hiểu sâu hơn có thể xem bài báo khoa học <a href="http://www.sciencedirect.com/science/article/pii/S0167865504000996">Cluster center initialization algorithm for K-means clustering</a>.</p>
  </li>
</ul>

<!-- ========================== New Heading ==================== -->
<p><a name="cac-cluster-can-co-so-luong-diem-gan-bang-nhau"></a></p>

<h4 id="các-cluster-cần-có-só-lượng-điểm-gần-bằng-nhau">Các cluster cần có só lượng điểm gần bằng nhau</h4>

<p>Dưới đây là một ví dụ với 3 cluster với 20, 50, và 1000 điểm. Kết quả cuối cùng không chính xác.</p>

<div class="imgcap">
<img src="/assets/kmeans/kmeans_unbalanced.gif" align="center" />
</div>

<!-- ========================== New Heading ==================== -->
<p><a name="cac-cluster-can-co-dang-hinh-tron"></a></p>

<h4 id="các-cluster-cần-có-dạng-hình-tròn">Các cluster cần có dạng hình tròn</h4>
<p>Tức các cluster tuân theo phân phối chuẩn và ma trận hiệp phương sai là ma trận đường chéo có các điểm trên đường chéo giống nhau.</p>

<p>Dưới đây là 1 ví dụ khi 1 cluster có dạng hình dẹt.</p>

<div class="imgcap">
<img src="/assets/kmeans/kmeans_diffcov.gif" align="center" />
</div>

<!-- ========================== New Heading ==================== -->
<p><a name="khi-mot-cluster-nam-phia-trong--cluster-khac"></a></p>

<h4 id="khi-một-cluster-nằm-phía-trong-1-cluster-khác">Khi một cluster nằm phía trong 1 cluster khác</h4>
<p>Đây là ví dụ kinh điển về việc K-means clustering không thể phân cụm dữ liệu. Một cách tự nhiên, chúng ta sẽ phân ra thành 4 cụm: mắt trái, mắt phải, miệng, xung quanh mặt. Nhưng vì mắt và miệng nằm trong khuôn mặt nên K-means clustering không thực hiện được:</p>

<div class="imgcap">
<img src="/assets/kmeans/smile_face.png" align="center" />
</div>
<!-- ========================== New Heading ==================== -->
<p><a name="-tai-lieu-tham-khao"></a></p>

<!-- ### Khoảng cách không phải Euclid -->
<!-- Trong nhiều trường hợp, chúng ta có thể định -->

<p>Mặc dù có những hạn chế, K-means clustering vẫn cực kỳ quan trọng trong Machine Learning và là nền tảng cho nhiều thuật toán phức tạp khác sau này. Chúng ta cần bắt đầu từ những thứ đơn giản. <em>Simple is best!</em></p>

<!-- ========================== New Heading ==================== -->
<p><a name="-tai-lieu-tham-khao"></a></p>

<h2 id="5-tài-liệu-tham-khảo">5. Tài liệu tham khảo</h2>

<p><a href="http://scikit-learn.org/stable/auto_examples/text/document_clustering.html">Clustering documents using k-means</a></p>

<p><a href="https://en.wikipedia.org/wiki/Voronoi_diagram">Voronoi Diagram - Wikipedia</a></p>

<p><a href="http://www.sciencedirect.com/science/article/pii/S0167865504000996">Cluster center initialization algorithm for K-means clustering</a></p>

<p><a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">Visualizing K-Means Clustering</a></p>

<p><a href="http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html">Visualizing K-Means Clustering - Standford</a></p>


</div>

<hr> 
<em>Nếu có câu hỏi, Bạn có thể để lại comment bên dưới hoặc trên <a href = "https://www.facebook.com/groups/257768141347267/">Forum</a> để nhận được câu trả lời sớm hơn.</em>
<br>
<em>Bạn đọc có thể ủng hộ blog qua <a href = "/buymeacoffee/">'Buy me a cofee'</a> ở góc trên bên trái của blog.
</em>

<br>
<em>Tôi vừa hoàn thành cuốn ebook 'Machine Learning cơ bản', bạn có thể đặt sách <a href = "/ebook/">tại đây</a>.

Cảm ơn bạn.</em>

<hr>

<!-- previous and next posts -->
<div class="PageNavigation">
   
      <a class="prev" style = "color: #204081;" href="/2016/12/28/linearregression/">&laquo; Bài 3: Linear Regression</a>
   
   
      <a class="next" style = "float: right; color: #204081;" href="/2017/01/04/kmeans2/">Bài 5: K-means Clustering: Simple Applications &raquo;</a>
   
</div>


<!-- disqus comments -->

      <hr>
   
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname  = 'tiepvu';
  var disqus_identifier = 'tiepvupsu.github.io' + '/2017/01/01/kmeans/';

  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    <script id="dsq-count-scr" src="//tiepvu.disqus.com/count.js" async></script>





   <!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11213319; 
var sc_invisible=0; 
var sc_security="9fb5c98f"; 
var sc_text=2; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("Total visits: <sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'> </"+"script>");
</script>
<noscript><div class="statcounter"><a title="web analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11213319/0/9fb5c98f/0/" alt="web
analytics"></a> </div></noscript>
<!-- End of StatCounter Code for Default Guide -->

<!-- <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- 
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?...">
</script> -->
	        </div>
	        <div class="col-md-2 hidden-xs hidden-sm">
	        	
          <!-- Google search -->
<!--           <table border="0">
          <div id = "top-widget" style="width: 252px; margin-left: -13.5px; margin-top: -10px; margin-bottom: -15px;">
         <script>
           (function() {
             var cx = '012053542614118746585:ktgei4l2oek';
             var gcse = document.createElement('script');
             gcse.type = 'text/javascript';
             gcse.async = true;
             gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
             var s = document.getElementsByTagName('script')[0];
             s.parentNode.insertBefore(gcse, s);
           })();
         </script>
         <gcse:search></gcse:search>
          </div>
          </table> -->

          

         <!--  
          <nav>
          
            <div class="header">Latest by category</div>
            <ul>
              
                
              
                
              
                
                  
                    <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif; color: #204081;" href="/2017/01/04/kmeans2/">Bài 5: K-means Clustering: Simple Applications</a></li>
                  
                    <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif; color: #204081;" href="/2017/01/01/kmeans/">Bài 4: K-means Clustering</a></li>
                  
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
            </ul>
          </nav>
          



          <nav>
            <div class="header">Latest</div>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/lifesofar2/">Con đường học PhD của tôi</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/10/03/conv2d">Bài 37: Tích chập hai chiều</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/09/11/forum/">Giới thiệu Diễn đàn Machine Learning cơ bản</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/07/06/deeplearning/">Bài 36. Giới thiệu về Keras</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/06/22/deeplearning/">Bài 35: Lược sử Deep Learning</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/03/22/phuonghoagiang/">Bạn đọc viết: Con đường học Khoa học dữ liệu của một sinh viên Kinh tế</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/01/14/id3/">Bài 34: Decision Trees (1): Iterative Dichotomiser 3</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/08/31/evaluation/">Bài 33: Các phương pháp đánh giá một hệ thống phân lớp</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/10/20/fundaml_vectors/">FundaML 3: Làm việc với các mảng ngẫu nhiên</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/10/20/fundaml_matrices/">FundaML 2: Làm việc với ma trận</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/10/12/fundaml_vectors/">FundaML 1: Làm việc với mảng một chiều</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/09/24/fundaml/">Giới thiệu trang web FundaML.com</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/08/08/nbc/">Bài 32: Naive Bayes Classifier</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/08/05/phdlife/">PhD life 1: Quá trình viết và nhận xét các bài báo khoa học</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/07/17/mlemap/">Bài 31: Maximum Likelihood và Maximum A Posteriori estimation</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/lifesofar/">Con đường học Toán của tôi</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/07/09/prob/">Bài 30: Ôn tập Xác Suất cho Machine Learning</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/07/02/tl/">Quick Note 2: Transfer Learning cho bài toán phân loại ảnh</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/06/30/lda/">Bài 29: Linear Discriminant Analysis</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/06/22/qns1/">Quick Notes 1</a></li>
              
            </ul>
          </nav> -->

          <aside class = "social">
          <div class = "header">Share</div>
          <div class="share-page">
    <!-- <b>Share this on:</b>  <br> -->

    <!-- Facebook -->
    <!-- <a href="https://facebook.com/sharer/sharer.php?u=https://machinelearningcoban.com/2017/01/01/kmeans/" rel="nofollow" target="_blank" title="Share on Facebook"><img src = "/assets/images/facebook.png" width="25"></a> -->

    <div class="fb-share-button" data-href="https://machinelearningcoban.com/2017/01/01/kmeans/" data-layout="button_count" data-size="small" data-mobile-iframe="true"><a class="fb-xfbml-parse-ignore" target="_blank" href="https://facebook.com/sharer/sharer.php?u=https://machinelearningcoban.com/2017/01/01/kmeans/">Share</a></div>


    <!-- Twitter -->
    <!-- <a href="https://twitter.com/intent/tweet?text=Bài 4: K-means Clustering&url=https://machinelearningcoban.com/2017/01/01/kmeans/&via=&related=" rel="nofollow" target="_blank" title="Share on Twitter" width="25" ><img src = "/assets/images/twitter.png" width="25"></a> -->

    <!-- Google -->
    <!-- <a href="https://plus.google.com/share?url=https://machinelearningcoban.com/2017/01/01/kmeans/" rel="nofollow" target="_blank" title="Share on Google+"><img src = "/assets/images/google.png" width="25"></a> -->

    
    <!-- LinkedIn -->
    <!-- <a href="http://www.linkedin.com/shareArticle?mini=true&amp;url=https://machinelearningcoban.com/2017/01/01/kmeans/" target="_blank"> <img src="/assets/images/linkedin.png" alt="LinkedIn" width="25"/> -->
    <!-- </a> -->

    <!-- Email -->
    <a href="/cdn-cgi/l/email-protection#536c002631393630276e003a3e233f3673003b32213673112627273c3d2075323e2368113c372a6e1a766163203224766163273b3a20766163323d37766163273b3c26343b277661633c357661632a3c2672766163733b27272320697c7c3e32303b3a3d363f3632213d3a3d34303c31323d7d303c3e7c616362647c63627c63627c383e36323d207c">
        <img src="/assets/images/email.png" alt="Email" width="25"/>
    </a>
    <!-- Print -->
    <a href="javascript:;" onclick="window.print()">
        <img src="/assets/images/print.png" alt="Print" width="25"/>
    </a>
   </div>
          </aside>
          
           <nav>
            <div class="header">Diễn đàn</div>
            <a   href="https://forum.machinelearningcoban.com">
            <img width="100%" src="/assets/latex/new_logo9-2.png"/>  </a>
          </nav>
          
          <nav>
            <div class="header">Interactive Learning</div>
            <a   href="https://fundaml.com">
            <img width="100%" src="/images/fundaml_web.png"/>  </a>
          </nav>

          <nav>
          <div class = "header" with = "100%">Facebook page</div>
          <!-- <a href = "https://www.facebook.com/machinelearningbasicvn/" target="_blank" title="Follow us"><img src = "/assets/images/facebook.png" width="30"></a> -->
          <!-- facebook page -->

         <div class="fb-page" data-href="https://www.facebook.com/machinelearningbasicvn/" data-width="250" data-small-header="false" data-adapt-container-width="true" data-hide-cover="false" data-show-facepile="false"><blockquote cite="https://www.facebook.com/machinelearningbasicvn/" class="fb-xfbml-parse-ignore" ><a style = "color: #204081" href="https://www.facebook.com/machinelearningbasicvn/">Machine Learning cơ bản</a></blockquote></div>
          <!--end facebook page -->

          </nav>
          <nav>
            <div class="header">Facebook group</div>
            <a   href="https://www.facebook.com/groups/257768141347267/">
            <img width="100%" src="/assets/14_mlp/multi_layers.png"/>  </a>
          </nav>
          


          <nav>

          <div class="header">Recommended books</div>
            <ul>
              <li> <a style="text-align: left; color: #074B80;" href="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjd7Y_Q-tzTAhVISyYKHUXyCekQFggvMAA&url=http%3A%2F%2Fusers.isr.ist.utl.pt%2F~wurmd%2FLivros%2Fschool%2FBishop%2520-%2520Pattern%2520Recognition%2520And%2520Machine%2520Learning%2520-%2520Springer%2520%25202006.pdf&usg=AFQjCNEVQzQ_dEpxG4P7NamTWUXnVXzCng&sig2=H1WVtom4rq3uh8UfbGX4oA">"Pattern recognition and Machine Learning.", C. Bishop </a></li>

              <li> <a style="text-align: left; color: #074B80;" href="https://github.com/tpn/pdfs/blob/master/The%20Elements%20of%20Statistical%20Learning%20-%20Data%20Mining%2C%20Inference%20and%20Prediction%20-%202nd%20Edition%20(ESLII_print4).pdf">"The Elements of Statistical Learning", T. Hastie et al.  </a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://www.computervisionmodels.com/">"Computer Vision:  Models, Learning, and Inference", Simon J.D. Prince </a></li>

              <li> <a style="text-align: left; color: #074B80;" href="https://stanford.edu/~boyd/cvxbook/">"Convex Optimization", Boyd and Vandenberghe</a></li>

            </ul>
          </nav>

          <nav>
          <div class="header">Recommended courses</div>

          <ul>
              <li> <a style="text-align: left; color: #074B80;" href="https://www.coursera.org/learn/machine-learning?utm_source=gg&utm_medium=sem&campaignid=693373197&adgroupid=36745103515&device=c&keyword=machine%20learning%20andrew%20ng&matchtype=e&network=g&devicemodel=&adpostion=1t1&creativeid=156061453588&hide_mobile_promo&gclid=Cj0KEQjwt6fHBRDtm9O8xPPHq4gBEiQAdxotvNEC6uHwKB5Ik_W87b9mo-zTkmj9ietB4sI8-WWmc5UaAi6a8P8HAQ">"Machine Learning", Andrew Ng </a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://web.stanford.edu/class/cs224n/">CS224n: Natural Language Processing with Deep Learning</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://cs231n.stanford.edu/">CS231n: Convolutional Neural Networks for Visual Recognition</a></li>           

              <li> <a style="text-align: left; color: #074B80;" href="http://web.stanford.edu/class/cs246/">CS246: Mining Massive Data Sets</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://web.stanford.edu/class/cs20si/syllabus.html">CS20SI: Tensorflow for Deep Learning Research </a></li>

              <li> <a style="text-align: left; color: #074B80;" href="https://www.edx.org/course/introduction-computer-science-mitx-6-00-1x-10">Introduction to Computer Science and Programming Using Python</a></li>           

            </ul>
          </nav>

          <nav>
          <div class="header">Others</div>
          <ul>
              <li> <a style="text-align: left; color: #074B80;" href="https://github.com/ZuzooVn/machine-learning-for-software-engineers">Top-down learning path: Machine Learning for Software Engineers</a></li>
              
              <li> <a style="text-align: left; color: #074B80;" href="/2017/02/02/howdoIcreatethisblog/">Blog này được tạo như thế nào?</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://thepresentwriter.com/chung-toi-da-apply-va-hoc-tien-si-nhu-the-nao-phan-1/">Chúng tôi đã apply và học tiến sỹ như thế nào? (1/2)</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://thepresentwriter.com/chung-toi-da-apply-va-hoc-tien-si-nhu-the-nao-phan-2/">Chúng tôi đã apply và học tiến sỹ như thế nào? (2/2)</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://machinelearningmastery.com/inspirational-applications-deep-learning/">8 Inspirational Applications of Deep Learning</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="https://ccrma.stanford.edu/~dattorro/matrixcalc.pdf">Matrix calculus</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="https://github.com/aymericdamien/TensorFlow-Examples">TensorFlow-Examples</a></li>
              
              <li> <a style="text-align: left; color: #074B80;" href="https://www.forbes.com/sites/quora/2017/04/05/eight-easy-steps-to-get-started-learning-artificial-intelligence/#53c29fa5b117">Eight Easy Steps To Get Started Learning Artificial Intelligence</a></li>
              <li> <a style="text-align: left; color: #074B80;" href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">The 9 Deep Learning Papers You Need To Know About</a></li>

                     

            </ul>
          </nav>
          <!-- <img style = "transform: scaleX(1); width:100%; margin-left:00px;position: absolute;" src = "/images/mai.jpg"> -->
         
         <!--   
            <nav>
              <div class="header">Previous by date</div>
              <ul>
                <li><a style="text-align: left; font-family: 'Roboto Condensed', sans-serif; color: #074B80;" href="/2016/12/28/linearregression/">Bài 3: Linear Regression</a></li>
              </ul>
            </nav>
           
           
            <nav>
              <div class="header">Next by date</div>
              <ul>
                <li><a style="text-align: left; font-family: 'Roboto Condensed', sans-serif; color: #204081;" href="/2017/01/04/kmeans2/">Bài 5: K-means Clustering: Simple Applications</a></li>
              </ul>
            </nav>
            -->
	        <!-- <img style = "transform: scaleX(1); width:250%; margin-left:-100px;" src = "/images/dao.jpg"> -->
	        <!-- <a href ="https://www.facebook.com/masspvn/?fref=nf&pnref=story">MaSSP</a> -->
	        </div>
      	</div>
    </div>
<script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script></body>

</html>
